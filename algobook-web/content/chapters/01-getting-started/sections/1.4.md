# Data Types: From Bits to Information

We have established that computers operate using binary foundations—every piece of information reduces to patterns of 0s and 1s. However, a critical question remains: how do computers distinguish between different kinds of information? The binary pattern $01000001$ could represent the number 65, the character 'A', or part of a floating-point number. The key insight is that **data types** provide systematic interpretation schemes that give meaning to bit patterns by defining what operations can be performed on them.

This section explores how fundamental data types emerge from binary representations and examines the hardware mechanisms that enable computation on these abstract entities. Understanding data types reveals how the same underlying bits can represent radically different information depending on the interpretation framework applied to them.

## The Interpretation Problem

Consider the 8-bit pattern: $01000001_2$

Without additional context, this bit sequence has no inherent meaning. However, different **interpretation schemes** can extract vastly different information:

- **As an unsigned integer**: $01000001_2 = 65_{10}$
- **As an ASCII character**: $01000001_2 = \text{'A'}$
- **As a Boolean array**: $[0, 1, 0, 0, 0, 0, 0, 1]$
- **As part of larger data**: Most significant byte of a 32-bit integer

**Data types** solve this interpretation problem by establishing systematic rules that define:
1. **Representation**: How bit patterns encode specific information
2. **Operations**: What computations can be performed on the data
3. **Behavior**: How operations transform the bit patterns

The same sequence of bits becomes meaningful only within a specific data type context that provides both interpretation and operational definitions.

## Universal Data Types

### Integers: Representing Whole Numbers

**Unsigned Integers:**
The most straightforward data type uses the binary number representation we established in Section 1.3. An $n$-bit unsigned integer can represent exactly $2^n$ values in the range $[0, 2^n - 1]$.

**Examples:**
- **8-bit unsigned**: Range $[0, 255]$, pattern $11111111_2 = 255_{10}$
- **16-bit unsigned**: Range $[0, 65535]$, pattern $1111111111111111_2 = 65535_{10}$
- **32-bit unsigned**: Range $[0, 4294967295]$

**Signed Integers: Two's Complement Representation:**
To represent negative numbers, computer systems use **two's complement** notation, which provides several mathematical advantages. In an $n$-bit two's complement system:

- **Positive numbers** ($0$ to $2^{n-1} - 1$): Use standard binary representation
- **Negative numbers** ($-2^{n-1}$ to $-1$): Use complemented representation

**Two's Complement Algorithm:**
To represent negative number $-N$ in two's complement:
1. **Find the binary representation** of the positive number $N$
2. **Flip all bits** (0 becomes 1, 1 becomes 0)
3. **Add 1** to the result

**Example: Representing $-5$ in 8-bit two's complement**
$$
\begin{align}
5_{10} &= 00000101_2 \\
\text{Flip bits: } &= 11111010_2 \\
\text{Add 1: } &= 11111011_2 = -5 \text{ in two's complement}
\end{align}
$$

**Mathematical Properties of Two's Complement:**
- **Single zero representation**: $00000000_2 = 0$
- **Symmetric addition**: $N + (-N) = 0$ with natural overflow
- **Range**: $[-2^{n-1}, 2^{n-1} - 1]$ for $n$-bit representation
- **Sign bit**: Most significant bit indicates sign (0 = positive, 1 = negative)

```quiz
id: twos-complement
question: "What is $-12$ represented in 8-bit two's complement?"
options:
  - id: a
    text: "$10001100_2$"
    correct: false
    explanation: "This is one's complement (just flipping bits). You need to add 1 after flipping."
  - id: b
    text: "$11110100_2$"
    correct: true
    explanation: "Correct! $12 = 00001100_2$, flip bits: $11110011_2$, add 1: $11110100_2$"
  - id: c
    text: "$11110011_2$"
    correct: false
    explanation: "This is one's complement of 12. Two's complement requires adding 1."
  - id: d
    text: "$01110100_2$"
    correct: false
    explanation: "This would be a positive number (sign bit is 0)."
```

### Floating-Point Numbers: Representing Real Values

Real numbers require more sophisticated representation schemes because they must encode both **magnitude** and **fractional components** within finite bit patterns.

**IEEE 754 Single-Precision Format (32 bits):**

The IEEE 754 standard divides 32 bits into three components:

$$\text{Value} = (-1)^{\text{sign}} \times 1.M \times 2^{E-127}$$

Where:
- **Sign bit (1 bit)**: 0 = positive, 1 = negative
- **Exponent (8 bits)**: Biased by 127, range $[1-127, 254-127] = [-126, 127]$
- **Mantissa/Fraction (23 bits)**: Fractional part, with implicit leading 1

**Example: Representing $6.5_{10}$ in IEEE 754**

First, convert to binary: $6.5_{10} = 110.1_2 = 1.101_2 \times 2^2$

- **Sign**: 0 (positive)
- **Exponent**: $2 + 127 = 129 = 10000001_2$
- **Mantissa**: $.101$ (pad to 23 bits): $10100000000000000000000_2$

**IEEE 754 representation**: $0\,10000001\,10100000000000000000000_2$

**Special Values:**
- **Zero**: Exponent = 0, Mantissa = 0
- **Infinity**: Exponent = 255, Mantissa = 0
- **NaN** (Not a Number): Exponent = 255, Mantissa ≠ 0

### Characters and Text: ASCII and Unicode

**ASCII (7-bit) Character Encoding:**
ASCII establishes fixed correspondence between 7-bit patterns and printable characters:

```table
title: ASCII Character Categories
headers: ["Range", "Characters", "Example"]
rows:
  - ["0-31", "Control characters", "Newline (10), Tab (9)"]
  - ["32-47", "Punctuation/symbols", "Space (32), '!' (33)"]
  - ["48-57", "Digits 0-9", "'0' (48), '9' (57)"]
  - ["65-90", "Uppercase A-Z", "'A' (65), 'Z' (90)"]
  - ["97-122", "Lowercase a-z", "'a' (97), 'z' (122)"]
```

**Extended Character Encodings:**
- **UTF-8**: Variable-length encoding (1-4 bytes) that extends ASCII
- **UTF-16**: 16-bit encoding for broader international character support
- **UTF-32**: 32-bit encoding providing direct Unicode code point access

### Boolean Values: Truth and Falsehood

Boolean data types implement logical truth values using minimal bit representations:

- **False**: $0_2$ (any zero bit pattern)
- **True**: $1_2$ (any non-zero bit pattern, often just $1$)

This direct mapping enables efficient implementation of the logical operations from Section 1.3.

```widget
id: data-type-explorer
type: DataTypeVisualization
```

## Operations and Data Type Semantics

Data types derive their computational power from the **operations** they support. The same bit patterns behave completely differently under different operational interpretations.

### Arithmetic Operations on Integers

**Addition Implementation:**
Binary addition follows the same positional logic as decimal addition, with carrying between bit positions:

$$
\begin{align}
  &\phantom{+}\ 1011_2 \quad (11_{10}) \\
+ &\phantom{+}\ 1101_2 \quad (13_{10}) \\
\hline
  &\ 11000_2 \quad (24_{10})
\end{align}
$$

**Carry Logic:**
- $0 + 0 = 0$ (no carry)
- $0 + 1 = 1$ (no carry)
- $1 + 0 = 1$ (no carry)
- $1 + 1 = 10_2$ (result = 0, carry = 1)
- $1 + 1 + 1 = 11_2$ (result = 1, carry = 1)

**Subtraction via Two's Complement:**
Computer hardware implements subtraction as addition with the two's complement of the subtrahend:

$$A - B = A + (-B) = A + \text{twos\_complement}(B)$$

This unifies addition and subtraction into a single hardware operation.

**Integer Overflow:**
When arithmetic results exceed the representational capacity, **overflow** occurs:

**8-bit unsigned overflow:** $255 + 1 = 256$, but $256 > 2^8 - 1$, so result wraps to $0$
**8-bit signed overflow:** $127 + 1 = 128$, but $128 > 2^7 - 1$, so result wraps to $-128$

### Floating-Point Arithmetic Challenges

Floating-point operations introduce **approximation errors** due to limited precision:

**Precision Limitations:**
Not all real numbers can be represented exactly in finite binary formats. For example, $0.1_{10}$ becomes a repeating binary fraction $0.000110011..._2$, leading to approximation.

**Associativity Violations:**
$(a + b) + c \neq a + (b + c)$ in floating-point arithmetic due to rounding errors accumulating differently.

**Example Precision Issue:**
```text
0.1 + 0.2 = 0.30000000000000004  (in IEEE 754)
```

### String Operations: Character Sequence Manipulation

Strings combine multiple character codes into sequences, supporting operations like:

- **Concatenation**: Combining character sequences
- **Indexing**: Accessing individual characters by position
- **Searching**: Finding subsequences within larger strings
- **Comparison**: Lexicographic ordering based on character codes

## Memory Organization and Data Storage

### Memory as Addressable Storage

Computer memory organizes bits into **addressable units** called **bytes** (8-bit groups). Each byte has a unique **memory address** that enables direct access to stored data.

**Memory Layout Example:**
```text
Address  | Byte Content    | Interpretation
---------|----------------|----------------
0x1000   | 01000001       | 'A' or 65
0x1001   | 01000010       | 'B' or 66
0x1002   | 01000011       | 'C' or 67
0x1003   | 00000000       | NULL terminator
```

**Multi-byte Data Types:**
Larger data types span multiple consecutive bytes:

- **16-bit integer**: 2 consecutive bytes
- **32-bit integer**: 4 consecutive bytes
- **64-bit double**: 8 consecutive bytes

**Endianness: Byte Order Convention:**
Multi-byte values can be stored in different byte orders:

- **Big-endian**: Most significant byte first
- **Little-endian**: Least significant byte first

**Example: Storing $0x12345678$ (32-bit integer)**
```text
Big-endian:    [0x12][0x34][0x56][0x78]
Little-endian: [0x78][0x56][0x34][0x12]
```

### Registers: High-Speed Storage

**CPU registers** provide fastest data access, typically matching the processor's natural word size (32-bit or 64-bit). Registers store:

- **Operands** for arithmetic operations
- **Intermediate results** during computation
- **Memory addresses** for data access
- **Program counters** and control information

```quiz
id: memory-representation
question: "A 32-bit integer with value 1000 is stored in little-endian format starting at address 0x2000. What byte appears at address 0x2001?"
options:
  - id: a
    text: "$0x03$"
    correct: true
    explanation: "Correct! 1000 = $0x000003E8$. In little-endian: [0xE8][0x03][0x00][0x00], so address 0x2001 contains 0x03."
  - id: b
    text: "$0x00$"
    correct: false
    explanation: "This would be at address 0x2002 or 0x2003 in little-endian format."
  - id: c
    text: "$0xE8$"
    correct: false
    explanation: "This is at address 0x2000 (least significant byte first in little-endian)."
  - id: d
    text: "$0x10$"
    correct: false
    explanation: "This doesn't correspond to any part of 1000 in hexadecimal."
```

## Hardware Arithmetic Implementation

### Binary Addition Circuits

**Half Adder:**
Implements addition of two single bits, producing sum and carry outputs:

- **Sum** = $A \oplus B$ (XOR operation)
- **Carry** = $A \land B$ (AND operation)

```table
title: Half Adder Truth Table
headers: ["A", "B", "Sum", "Carry"]
rows:
  - ["0", "0", "0", "0"]
  - ["0", "1", "1", "0"]
  - ["1", "0", "1", "0"]
  - ["1", "1", "0", "1"]
```

**Full Adder:**
Extends half adder to include carry input from previous position:

- **Sum** = $A \oplus B \oplus C_{in}$
- **Carry** = $(A \land B) \lor (C_{in} \land (A \oplus B))$

**Ripple Carry Adder:**
Chains full adders to handle multi-bit addition, with carry propagating through each bit position sequentially.

### Multiplication Implementation

**Shift-and-Add Algorithm:**
Binary multiplication uses the distributive property:

$$1011_2 \times 101_2 = 1011_2 \times (1 \times 2^2 + 0 \times 2^1 + 1 \times 2^0)$$

**Implementation Steps:**
1. **Initialize** product = 0
2. **For each bit** in multiplier (right to left):
   - If bit = 1, add (multiplicand × 2^position) to product
   - Shift multiplicand left for next position

**Hardware Optimization:**
Modern processors use parallel multiplication circuits that compute multiple partial products simultaneously, dramatically reducing computation time.

### Division Implementation

**Division by Repeated Subtraction:**
Basic division algorithm repeatedly subtracts divisor from dividend:

```text
Algorithm: Integer Division
dividend = 19, divisor = 3
quotient = 0

While dividend ≥ divisor:
    dividend = dividend - divisor
    quotient = quotient + 1

Result: quotient = 6, remainder = 1
```

**Hardware Division:**
Processors implement optimized division using techniques like:
- **Restoring division**: Systematic subtraction with result restoration
- **Non-restoring division**: Conditional addition/subtraction
- **Newton-Raphson**: Iterative approximation for faster convergence

```quiz
id: hardware-arithmetic
question: "In a 4-bit ripple carry adder computing $1101_2 + 0011_2$, what is the carry output from the second bit position (bit 1)?"
options:
  - id: a
    text: "0"
    correct: false
    explanation: "Bit 1: 0 + 1 + 1(carry from bit 0) = 0 with carry 1."
  - id: b
    text: "1"
    correct: true
    explanation: "Correct! Bit 1: 0 + 1 + 1(carry from bit 0) = 10, so sum=0, carry=1."
  - id: c
    text: "Depends on input"
    correct: false
    explanation: "The inputs are fixed: $1101_2 + 0011_2$, so the carry is deterministic."
  - id: d
    text: "Undefined"
    correct: false
    explanation: "Carry propagation follows precise logic rules."
```

## Data Types as Abstraction Layers

Understanding data types reveals a fundamental principle of computer science: **systematic abstraction**. Each data type provides a layer of abstraction that:

**Encapsulates Representation:** Users work with integers, floats, and strings without managing individual bit patterns.

**Defines Valid Operations:** Type systems prevent meaningless operations (e.g., adding a number to a character).

**Enables Optimization:** Knowing data types allows compilers to choose optimal hardware instructions.

**Provides Safety:** Type checking catches errors before they become runtime failures.

### Type Systems and Program Correctness

**Static vs Dynamic Typing:**
- **Static typing**: Types determined at compile time, enabling early error detection
- **Dynamic typing**: Types determined at runtime, providing flexibility at the cost of runtime overhead

**Strong vs Weak Typing:**
- **Strong typing**: Strict type rules prevent implicit conversions
- **Weak typing**: Liberal type conversions, potentially hiding logical errors

```widget
id: type-conversion-explorer
type: DataTypeConverter
```

## The Universal Binary Foundation

This exploration of data types reveals a profound computational principle: **all information processing reduces to systematic manipulation of binary patterns**. Whether performing integer arithmetic, floating-point calculations, string processing, or logical operations, the underlying mechanism involves:

1. **Interpretation**: Data types provide systematic rules for interpreting bit patterns
2. **Operation**: Hardware circuits implement operations defined by the data type
3. **Transformation**: Operations produce new bit patterns according to type-specific rules
4. **Abstraction**: Higher-level constructs hide binary details while preserving operational semantics

**Algorithmic Implications:**
Understanding data types illuminates several important considerations for algorithm design:

- **Precision bounds**: Integer overflow and floating-point errors can affect algorithm correctness
- **Performance characteristics**: Different data types have different computational costs
- **Memory requirements**: Data type choice directly impacts storage needs
- **Hardware optimization**: Algorithms can exploit specific hardware capabilities for different types

The progression from bits through data types to high-level programming constructs demonstrates how systematic abstraction enables computational power. We begin with physical binary states, construct logical operations, build arithmetic capabilities, define data types with their operational semantics, and ultimately implement sophisticated algorithms.

## Summary

This section established how data types provide systematic interpretation frameworks for binary information:

**Data Types as Interpretation**: The same bit patterns acquire different meanings and behaviors depending on their data type context.

**Universal Representations**: Integers, floating-point numbers, characters, and Boolean values all reduce to systematic binary encoding schemes.

**Operational Semantics**: Data types define not just representation but also the valid operations and their computational behaviors.

**Hardware Implementation**: All data type operations ultimately translate to circuits manipulating binary patterns according to type-specific rules.

**Abstraction Layering**: Data types provide crucial abstraction layers that enable programming languages to offer convenient high-level constructs while maintaining computational efficiency.

The next chapter builds upon these data type foundations to explore how programming languages combine data types with control structures to implement the systematic algorithms developed in our earlier sections.