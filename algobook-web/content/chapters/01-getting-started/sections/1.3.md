# The Digital Foundation

We have established systematic methods for transforming informal thoughts into precise algorithmic specifications. However, a critical gap remains: how do we bridge from these human-readable instructions to the actual operations that computers can execute? This section explores the fundamental digital representations that enable computers to store, manipulate, and process the information our algorithms require.

## From Algorithms to Machine Instructions

A fundamental challenge emerges when we consider how computers execute our systematic algorithms: computers operate using electrical circuits that can distinguish only between two states—current flowing or not flowing, voltage high or low, magnetic field oriented north or south. Every computational operation, from simple arithmetic to complex algorithm execution, must ultimately be expressed using just these two primitive states.

**The Digital Translation Problem:**

Human algorithms use rich abstractions: variables represent arbitrary values, comparisons evaluate relationships between abstract quantities, and control structures direct execution flow through logical decisions. Digital computers, however, operate through billions of microscopic switches that can only be "on" or "off". The remarkable achievement of computer science lies in constructing systematic methods for representing arbitrary information and complex operations using only these binary distinctions.

This binary foundation imposes no fundamental limitations on computational power—any algorithm that can be expressed using our three logic types can be implemented using binary representations. The binary system provides a universal language for computation, capable of representing numbers, text, images, sounds, and even the algorithms themselves that manipulate this information.

## The Bit: Atomic Unit of Digital Information

The **bit** (binary digit) serves as the fundamental unit of digital information—the smallest possible distinction that can be stored or communicated. A single bit represents the outcome of any yes/no question, the state of any on/off switch, or the choice between any two alternatives.

**Bit as Pure Information:**

From an information-theoretic perspective, one bit represents exactly the amount of information needed to choose between two equally likely alternatives. This mathematical precision makes bits ideal building blocks for larger information structures. Whether representing a coin flip result, a true/false answer, or the state of a circuit element, one bit captures exactly one fundamental binary distinction.

**Physical Implementation of Bits:**

Modern computers implement bits through various physical mechanisms, each exploiting binary states in different materials:

- **Electronic circuits**: High voltage (≈3.3V) represents 1, low voltage (≈0V) represents 0
- **Magnetic storage**: North magnetic pole orientation represents 1, south orientation represents 0
- **Optical storage**: Reflective surface represents 1, non-reflective surface represents 0
- **Flash memory**: Trapped electric charge represents 1, no trapped charge represents 0

The choice of physical implementation affects speed, durability, and cost, but all representations are logically equivalent—they provide reliable methods for storing and retrieving binary distinctions.

**Bit Notation and Convention:**

By universal convention, bits are represented using the symbols 0 and 1. This notation does not imply mathematical properties—the symbols serve as labels for the two possible states. The choice of which physical state corresponds to 1 versus 0 is arbitrary but must be consistent throughout a system.

```table
title: Bit Value Meanings
headers: ["Bit Value", "Meaning"]
rows:
  - ["0", "False, Off, Low, No, Absent"]
  - ["1", "True, On, High, Yes, Present"]
```

Individual bits can answer only the simplest questions, but they also serve as the foundation for logical computation itself.

## Bits as Boolean Logic in Action

Beyond storage, bits naturally implement the logical operations that form the foundation of algorithmic reasoning. The three fundamental Boolean operations (AND, OR, NOT) correspond directly to physical circuit operations, creating a seamless connection between logical reasoning and physical computation.

### Fundamental Logical Operations

**AND Operation (∧):**
The AND operation produces 1 only when both input bits are 1, implementing the logical concept "both conditions must be true."

```table
title: AND Operation Truth Table
headers: ["Input A", "Input B", "A ∧ B"]
rows:
  - ["0", "0", "0"]
  - ["0", "1", "0"]
  - ["1", "0", "0"]
  - ["1", "1", "1"]
```

**Physical implementation**: Electric circuits where current flows through the output only when both input switches are closed.

**OR Operation (∨):**
The OR operation produces 1 when at least one input bit is 1, implementing "at least one condition must be true."

```table
title: OR Operation Truth Table
headers: ["Input A", "Input B", "A ∨ B"]
rows:
  - ["0", "0", "0"]
  - ["0", "1", "1"]
  - ["1", "0", "1"]
  - ["1", "1", "1"]
```

**Physical implementation**: Parallel circuits where current flows through the output when either input switch is closed.

**NOT Operation (¬):**
The NOT operation flips the bit value, implementing logical negation.

```table
title: NOT Operation Truth Table
headers: ["Input A", "¬A"]
rows:
  - ["0", "1"]
  - ["1", "0"]
```

**Physical implementation**: Inverting circuits that output high voltage when input is low, and vice versa.

### Extended Operations from Fundamentals

These three primitive operations combine to create more sophisticated logical functions:

**XOR (Exclusive OR) Operation (⊕):**
Produces 1 when inputs differ, implementing "exactly one condition must be true."

A ⊕ B = (A ∨ B) ∧ ¬(A ∧ B)

```table
title: XOR Operation Truth Table
headers: ["Input A", "Input B", "A ⊕ B"]
rows:
  - ["0", "0", "0"]
  - ["0", "1", "1"]
  - ["1", "0", "1"]
  - ["1", "1", "0"]
```

XOR proves essential for error detection, encryption, and efficient algorithm implementation.

**NAND (NOT AND) Operation (↑):**
Produces 0 only when both inputs are 1, implementing "not both conditions are true."

A ↑ B = ¬(A ∧ B)

```table
title: NAND Operation Truth Table
headers: ["Input A", "Input B", "A ↑ B"]
rows:
  - ["0", "0", "1"]
  - ["0", "1", "1"]
  - ["1", "0", "1"]
  - ["1", "1", "0"]
```

NAND operations are fundamental building blocks in digital circuit design.

**NOR (NOT OR) Operation (↓):**
Produces 1 only when both inputs are 0, implementing "neither condition is true."

A ↓ B = ¬(A ∨ B)

```table
title: NOR Operation Truth Table
headers: ["Input A", "Input B", "A ↓ B"]
rows:
  - ["0", "0", "1"]
  - ["0", "1", "0"]
  - ["1", "0", "0"]
  - ["1", "1", "0"]
```

NOR operations are also fundamental building blocks in digital circuit design.

**Algorithmic Connection to Logical Operations:**

These bit-level operations directly implement the conditional logic found in our algorithmic control structures. The "IF condition THEN action" pattern translates to circuits that enable specific operations only when condition bits satisfy particular logical combinations. Similarly, loop continuation depends on condition bits that control circuit paths through the processor.

```widget
id: boolean-logic-fundamentals
type: BooleanLogicCalculator
```

```quiz
id: universal-gates
question: "Which of the following Boolean operations can be used to implement ALL other Boolean functions?"
options:
  - id: a
    text: "AND (∧) alone"
    correct: false
    explanation: "AND cannot implement NOT by itself, so it cannot be universal."
  - id: b
    text: "OR (∨) alone"
    correct: false
    explanation: "OR cannot implement NOT by itself, so it cannot be universal."
  - id: c
    text: "NAND (↑) alone"
    correct: true
    explanation: "Correct! NAND is universal - any Boolean function can be built using only NAND operations. For example: NOT A = A ↑ A, and A ∧ B = (A ↑ B) ↑ (A ↑ B)."
  - id: d
    text: "XOR (⊕) alone"
    correct: false
    explanation: "XOR cannot implement AND or OR operations by itself."
```

```quiz
id: boolean-operations
question: "What is the result of the operation (1 ∧ 0) ∨ (¬1 ⊕ 0)?"
options:
  - id: a
    text: "0"
    correct: true
    explanation: "Step by step: (1 ∧ 0) = 0, (¬1) = 0, (0 ⊕ 0) = 0, so we have 0 ∨ 0 = 0."
  - id: b
    text: "1"
    correct: false
    explanation: "Let's trace through: (1 ∧ 0) = 0, ¬1 = 0, then (0 ⊕ 0) = 0, so 0 ∨ 0 = 0."
  - id: c
    text: "The operation is undefined"
    correct: false
    explanation: "All Boolean operations are well-defined for binary inputs."
  - id: d
    text: "It depends on the implementation"
    correct: false
    explanation: "Boolean logic operations have consistent, universal definitions."
```

## Combining Bits for Richer Information

While individual bits can represent only binary choices, systematic combinations of bits enable representation of arbitrary information. The key insight lies in positional notation—assigning specific meaning to each bit position within a group, creating structured encodings that can represent numbers, symbols, and complex data.

### Binary Number Representation

Why do we represent the number "one hundred twenty-three" as 123? The answer lies in positional notation—each digit position represents a different power of 10. The rightmost digit counts ones ($10^0$), the next counts tens ($10^1$), then hundreds ($10^2$), and so on. We write 123 because:

$$
123 = 1 \times 10^2 + 2 \times 10^1 + 3 \times 10^0 = 100 + 20 + 3
$$

This decimal system uses base 10, but the underlying principle works with any base. Computers use base 2 because their circuits can only distinguish between two states. For any binary string $b_n b_{n-1} \ldots b_1 b_0$ where each $b_i \in \{0,1\}$, we apply the same positional principle with powers of 2:

$$
\text{Value} = \sum_{i=0}^{n} b_i \cdot 2^i
$$

Each bit position $i$ contributes $b_i \cdot 2^i$ to the total value, where $b_i$ is the bit value (0 or 1) and $2^i$ is the positional weight.

**Binary to Decimal Conversion:**

This mathematical definition directly provides our conversion method. To convert any binary number to decimal, we evaluate the formula $\sum_{i=0}^{n} b_i \cdot 2^i$ by computing each term and summing the results.

To distinguish between number bases, we use subscript notation: $123_{10}$ means "123 in base 10" (decimal), while $101_2$ means "101 in base 2" (binary).

**Worked Example: Converting $10110101_2$ to Decimal**

Applying the formula $\sum_{i=0}^{7} b_i \cdot 2^i$:

$$
\begin{align}
10110101_2 &= 1 \cdot 2^7 + 0 \cdot 2^6 + 1 \cdot 2^5 + 1 \cdot 2^4 + 0 \cdot 2^3 + 1 \cdot 2^2 + 0 \cdot 2^1 + 1 \cdot 2^0 \\
&= 1 \cdot 128 + 0 \cdot 64 + 1 \cdot 32 + 1 \cdot 16 + 0 \cdot 8 + 1 \cdot 4 + 0 \cdot 2 + 1 \cdot 1 \\
&= 128 + 32 + 16 + 4 + 1 \\
&= 181_{10}
\end{align}
$$

```widget
id: simple-binary-converter
type: BinaryToDecimalConverter
```

**Decimal to Binary Conversion Method:**

Converting from decimal to binary requires finding the bit values $b_i$ in the representation $N = \sum_{i=0}^{n} b_i \cdot 2^i$. We use repeated division by 2 because of the mathematical property:

$$
N = b_0 + 2(b_1 + 2(b_2 + 2(b_3 + \ldots)))
$$

When we divide $N$ by 2:
- The remainder is $b_0$ (the least significant bit)
- The quotient is $b_1 + 2(b_2 + 2(b_3 + \ldots))$, which has the same structure

This recursive structure means each division extracts one bit and leaves a quotient ready for the next extraction:

1. **Divide the decimal number by 2**
2. **Record the remainder** (0 or 1) - this is the next bit $b_i$
3. **Use the quotient** as the new number to divide
4. **Repeat until the quotient becomes 0**
5. **Read the remainders from bottom to top** to get the binary result

**Worked Example: Converting $181_{10}$ to Binary**

$$
\begin{align}
181 \div 2 &= 90 \text{ remainder } 1 \\
90 \div 2 &= 45 \text{ remainder } 0 \\
45 \div 2 &= 22 \text{ remainder } 1 \\
22 \div 2 &= 11 \text{ remainder } 0 \\
11 \div 2 &= 5 \text{ remainder } 1 \\
5 \div 2 &= 2 \text{ remainder } 1 \\
2 \div 2 &= 1 \text{ remainder } 0 \\
1 \div 2 &= 0 \text{ remainder } 1
\end{align}
$$

Reading the remainders from bottom to top gives us $181_{10} = 10110101_2$.

```widget
id: decimal-to-binary-demo
type: DecimalToBinaryConverter
```

```quiz
id: binary-conversion
question: "What is the decimal value of the binary number 1101?"
options:
  - id: a
    text: "11"
    correct: false
    explanation: "This converts each digit separately rather than using positional notation."
  - id: b
    text: "13"
    correct: true
    explanation: "Correct: 1×2³ + 1×2² + 0×2¹ + 1×2⁰ = 8 + 4 + 0 + 1 = 13"
  - id: c
    text: "15"
    correct: false
    explanation: "This would be 1111 in binary."
  - id: d
    text: "9"
    correct: false
    explanation: "This would be 1001 in binary."
```

```quiz
id: decimal-to-binary-conversion
question: "Convert the decimal number 26 to binary."
options:
  - id: a
    text: "$11010_2$"
    correct: true
    explanation: "Correct! Using repeated division: 26÷2=13 r0, 13÷2=6 r1, 6÷2=3 r0, 3÷2=1 r1, 1÷2=0 r1. Reading bottom to top: $11010_2$"
  - id: b
    text: "$10110_2$"
    correct: false
    explanation: "This would be 22 in decimal. Check your division steps carefully."
  - id: c
    text: "$11100_2$"
    correct: false
    explanation: "This would be 28 in decimal. Remember to use repeated division by 2."
  - id: d
    text: "$10010_2$"
    correct: false
    explanation: "This would be 18 in decimal. Make sure to record remainders correctly."
```

**Representational Capacity:**

An $n$-bit binary system can represent exactly $2^n$ distinct values in the range $[0, 2^n - 1]$. This follows from the fundamental counting principle: each of the $n$ positions can take one of 2 values, yielding $2^n$ total combinations.


```widget
id: binary-challenge
type: BinaryAdditionGame
```

### Hexadecimal as Binary Shorthand

Computer systems commonly work with **bytes**—groups of 8 bits that can represent values from 0 to 255. However, writing bytes in binary becomes cumbersome: the byte representing 181 requires the full string $10110101_2$, while 255 requires $11111111_2$.

A natural solution emerges when we split a byte into two halves of 4 bits each. Since 4 bits can represent exactly $2^4 = 16$ distinct values (0 through 15), we need a base-16 number system to represent each half compactly.

**Hexadecimal (base-16)** provides exactly this system. We use the familiar digits 0-9 for values 0 through 9, and extend with letters A-F to represent values 10 through 15:

```table
title: Hexadecimal-Binary Correspondence
headers: ["Hex", "Binary", "Decimal"]
rows:
  - ["0", "0000", "0"]
  - ["1", "0001", "1"]
  - ["2", "0010", "2"]
  - ["3", "0011", "3"]
  - ["4", "0100", "4"]
  - ["5", "0101", "5"]
  - ["6", "0110", "6"]
  - ["7", "0111", "7"]
  - ["8", "1000", "8"]
  - ["9", "1001", "9"]
  - ["A", "1010", "10"]
  - ["B", "1011", "11"]
  - ["C", "1100", "12"]
  - ["D", "1101", "13"]
  - ["E", "1110", "14"]
  - ["F", "1111", "15"]
```

Each hexadecimal digit represents exactly four binary bits, enabling efficient conversion and compact notation for binary data.

**Binary ↔ Hexadecimal Conversion:**

The byte-splitting approach we established makes binary-hexadecimal conversion remarkably straightforward. Since each hexadecimal digit represents exactly 4 bits, we can convert directly by grouping:

**Binary to Hexadecimal:**
1. Group the binary digits into sets of 4, starting from the right
2. Convert each 4-bit group to its corresponding hex digit
3. Combine the hex digits to form the final result

**Hexadecimal to Binary:**
1. Replace each hex digit with its 4-bit binary equivalent
2. Concatenate all the 4-bit groups
3. Remove any leading zeros if needed

For hexadecimal-decimal conversion, we use the same positional notation approach as with binary, but the direct binary↔hex conversion is far more important since hexadecimal serves primarily as compact binary notation.

**Example: Binary ↔ Hexadecimal Direct Conversion**

$$
\begin{align}
\text{Binary:} \quad &10110101\,11001110_2 \\
\text{Group by 4:} \quad &\underbrace{1011}_B \,\underbrace{0101}_5 \,\underbrace{1100}_C \,\underbrace{1110}_E \\
\text{Hexadecimal:} \quad &\text{B5CE}_{16}
\end{align}
$$

This direct correspondence makes hexadecimal ideal for representing binary data compactly while preserving the underlying bit structure.

```quiz
id: hexadecimal-conversion
question: "Convert the binary number $11010110_2$ to hexadecimal."
options:
  - id: a
    text: "$\\text{D6}_{16}$"
    correct: true
    explanation: "Correct! Group by 4 bits: 1101|0110 → D|6 → $\\text{D6}_{16}$"
  - id: b
    text: "$\\text{DA}_{16}$"
    correct: false
    explanation: "This would be $11011010_2$. Check your grouping of 4-bit segments."
  - id: c
    text: "$214_{10}$"
    correct: false
    explanation: "This is the decimal value, not hexadecimal notation."
  - id: d
    text: "$\\text{CB}_{16}$"
    correct: false
    explanation: "This would be $11001011_2$. Remember to group from right to left."
```


### Bitwise Operations: Boolean Logic on Binary Data

The Boolean operations we established earlier (AND, OR, XOR, NOT) extend naturally to entire binary numbers by applying the operation to corresponding bit positions. These **bitwise operations** form the foundation for many practical computing applications.

**Bitwise AND (&):** Apply AND to each bit position
$$1010_2 \text{ ∧ } 1100_2 = 1000_2$$

**Bitwise OR (|):** Apply OR to each bit position
$$1010_2 \text{ ∨ } 1100_2 = 1110_2$$

**Bitwise XOR (⊕):** Apply XOR to each bit position
$$1010_2 \text{ ⊕ } 1100_2 = 0110_2$$

**Practical Applications:**

**IP Network Masking (Bitwise AND):**
An **IP address** is a 32-bit number (4 bytes) that uniquely identifies a computer on the internet. Instead of writing this as one long binary number, it's typically represented as four decimal numbers separated by dots, where each number represents one byte (0-255). For example, $203.45.67.100$ represents the 32-bit binary number $11001011.00101101.01000011.01100100_2$.

When an organization receives IP addresses from their internet provider, they don't get random scattered addresses. Instead, they get a **block** of consecutive addresses that all share the same first several bits. For example, a university might receive all IP addresses from $203.45.67.0$ to $203.45.67.255$ — notice they all start with $203.45.67$. In this case, the first 24 bits are fixed for the network, while the remaining 8 bits can be allocated to individual devices within the network, allowing for $2^8 = 256$ addresses. This is typically written as $203.45.67.0/24$, where the "/24" indicates that the first 24 bits are fixed.

A larger company might receive a bigger block with only 20 fixed bits, such as all addresses from $198.51.96.0$ to $198.51.111.255$, written as $198.51.96.0/20$. Here, the first 20 bits identify the company's network, while the remaining 12 bits can accommodate $2^{12} = 4096$ individual devices.

In the first example, the shared 24 bits identify the organization's network, while the remaining 8 bits identify individual computers within that organization. This is like how all houses on "123 Main Street" share the street address, but have different apartment numbers.

A **subnet mask** is a binary pattern used to extract just the network portion from any IP address. The mask contains $1$s for the shared network bits and $0$s for the individual host bits. When you apply bitwise AND, the $1$s preserve the network bits while the $0$s "mask out" (zero out) the host bits.

The /24 notation directly corresponds to the subnet mask: /24 means the first 24 bits are $1$s, so the mask is $255.255.255.0 = 11111111.11111111.11111111.00000000_2$.

If a computer has an IP address, how do we obtain its **network address**? The **network address** is the identifier that represents the entire network block to which an IP address belongs — it's the IP address with all host bits set to zero.

**Example:**
- IP Address: $203.45.67.100 = 11001011.00101101.01000011.01100100_2$
- Subnet Mask for /24: $255.255.255.0 = 11111111.11111111.11111111.00000000_2$
- Network Address: IP ∧ Mask = $11001011.00101101.01000011.00000000_2 = 203.45.67.0$

The AND operation extracts exactly the network address ($203.45.67.0$) from the /24 block, telling us this computer belongs to the "$203.45.67.0/24$" network.

This operation is also used to test whether two IP addresses belong to the same network: if both IP addresses produce the same network address when AND-ed with the subnet mask, they're on the same network.

**File Permissions (Bitwise OR):**
Unix/Linux systems use three bits to represent file permissions: one bit each for **read**, **write**, and **execute**. Since each bit position has a specific decimal value (positions 2, 1, 0 correspond to values 4, 2, 1), we get: **read=4**, **write=2**, **execute=1**. To grant multiple permissions, we combine them using OR:

- Read permission: $100_2 = 4_{10}$
- Write permission: $010_2 = 2_{10}$
- Execute permission: $001_2 = 1_{10}$
- Read + Write: $100_2 \text{ ∨ } 010_2 = 110_2 = 6_{10}$
- All permissions: $100_2 \text{ ∨ } 010_2 \text{ ∨ } 001_2 = 111_2 = 7_{10}$

This is why commands like `chmod 755` work: the 7 represents owner permissions (4+2+1), while 5 represents group/other permissions (4+1).

Why use bitwise OR to combine permissions instead of other storage methods? Because it makes testing permissions extremely efficient. To test if someone has read permission, the system simply performs a bitwise AND with $100_2$ (the read flag): if the result is non-zero, read permission exists. For example, to check read permission on a file with permissions $110_2$ (read+write): $110_2 \text{ ∧ } 100_2 = 100_2 \neq 0$, confirming read access.

**Error Detection (Bitwise XOR):**
When data is transmitted over networks or stored on devices, bits can get corrupted due to electrical interference, hardware failures, or other issues. How do we detect if the received data is the same as what was sent? We need a **checksum** — a verification code that can detect errors.

XOR's property of revealing differences makes it ideal for this purpose. A simple checksum system works by XORing all data bits together:

- Original data: $1101$, $1010$, $0110$
- Checksum: $1101 \text{ ⊕ } 1010 \text{ ⊕ } 0110 = 0001$
- Send: data + checksum

When the receiver gets the data, they perform the same checksum calculation: XOR all the received data bits together. If no errors occurred, this calculated checksum should equal the transmitted checksum. For example:
- Received data: $1101$, $1010$, $0110$ (same as original)
- Calculated checksum: $1101 \text{ ⊕ } 1010 \text{ ⊕ } 0110 = 0001$
- Transmitted checksum: $0001$
- Since $0001 = 0001$, no errors detected.

If the calculated and transmitted checksums differ, an error occurred during transmission.

```quiz
id: bitwise-applications
question: "A file has permissions $101_2$ (read + execute). To add write permission while keeping existing permissions, which operation should be used?"
options:
  - id: a
    text: "AND with $010_2$"
    correct: false
    explanation: "AND would remove the existing permissions. We need to add, not filter."
  - id: b
    text: "OR with $010_2$"
    correct: true
    explanation: "Correct! OR with $010_2$ (write bit) gives $101_2 ∨ 010_2 = 111_2$, adding write permission while preserving read and execute."
  - id: c
    text: "XOR with $010_2$"
    correct: false
    explanation: "XOR would toggle the write bit, but we specifically want to add it."
  - id: d
    text: "Replace with $010_2$"
    correct: false
    explanation: "This would remove read and execute permissions, keeping only write."
```

**Algorithmic Foundation:**

These bitwise operations demonstrate how the logical reasoning patterns from our algorithms translate directly to efficient binary data manipulation. They bridge the gap between abstract logical operations and concrete computational techniques used throughout computer science.

## The Universal Digital Language

Binary representation provides the universal foundation that enables computers to execute the systematic algorithms we design. Every algorithmic operation—from simple variable assignment through complex conditional logic to iterative processing—ultimately translates to sequences of binary operations performed by electronic circuits.

This foundation demonstrates a profound principle: **systematic abstraction enables computational power**. We begin with physical binary states, construct logical operations, build arithmetic capabilities, encode complex data types, and ultimately implement sophisticated algorithms. Each layer builds systematically on the previous foundation while hiding implementation details from higher levels.

Understanding this digital foundation prepares us for the next crucial step: learning how programming languages provide convenient abstractions over binary operations, enabling practical implementation of our algorithmic specifications. The progression from binary circuits through logical operations to high-level programming constructs represents the systematic layering that makes modern computation possible.

## Summary

This section established the digital foundation underlying all computational operations:

**Binary as Universal Language**: All digital information reduces to systematic combinations of bits representing binary distinctions.

**Logical Operations**: Boolean logic operations (AND, OR, NOT, XOR) provide the primitive building blocks for implementing algorithmic conditional reasoning.

**Number Representation**: Binary and hexadecimal systems enable systematic representation of numerical data using positional notation.

**Practical Applications**: Bitwise operations enable efficient solutions for real-world problems including network routing, system permissions, and error detection.

The next section explores how programming languages provide systematic data types that build upon these binary foundations to represent different kinds of information.