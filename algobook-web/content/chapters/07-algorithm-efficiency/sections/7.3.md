# Mathematical Analysis: Big O Notation

You've learned to measure algorithm performance with timing and operation counting. But what if you need to predict performance **without running experiments?** **Big O notation is the mathematical language that describes how algorithms scale with input size.** It's the tool professionals use to analyze and compare algorithms.

## Learning Objectives

By the end of this section, you will:
- Understand Big O notation and what it represents
- Classify common algorithm patterns by their Big O complexity
- Analyze simple algorithms to determine their Big O
- Predict algorithm performance using Big O analysis
- Make informed decisions between algorithms based on their complexity

## What is Big O Notation?

Big O notation describes how an algorithm's performance grows as the input size increases. It focuses on the **dominant factor** that affects performance as data gets very large.

```python-execute
# Let's see why we focus on the dominant factor
def analyze_algorithm_components(n):
    """Break down an algorithm's operations into components"""
    
    constant_work = 5           # Always 5 operations
    linear_work = 3 * n         # Grows with n
    quadratic_work = 2 * n * n  # Grows with n²
    
    total_operations = constant_work + linear_work + quadratic_work
    
    print(f"Input size n = {n}")
    print(f"Constant work:  {constant_work}")
    print(f"Linear work:    {linear_work}")
    print(f"Quadratic work: {quadratic_work}")
    print(f"Total:          {total_operations}")
    print(f"Dominant factor: quadratic ({quadratic_work/total_operations*100:.1f}%)")
    print()

# Test with increasing sizes
for size in [10, 100, 1000]:
    analyze_algorithm_components(size)
```

```note title="Why Dominant Terms Matter"
As input size grows large, the highest-order term dominates all others. With n=1000, the quadratic term (2,000,000) overwhelms both the constant (5) and linear (3,000) terms. Big O focuses on this dominant behavior.
```

## Common Big O Complexities

Let's explore the most important complexity classes:

```python-execute
import math

def compare_complexities(n):
    """Compare different Big O complexities for input size n"""
    
    # Common complexity functions
    constant = 1                    # O(1)
    logarithmic = math.log2(n)     # O(log n)
    linear = n                     # O(n)
    linearithmic = n * math.log2(n) # O(n log n)
    quadratic = n * n              # O(n²)
    cubic = n * n * n              # O(n³)
    exponential = 2 ** min(n, 20)  # O(2^n) - limited to prevent overflow
    
    return {
        "O(1)": int(constant),
        "O(log n)": int(logarithmic),
        "O(n)": int(linear),
        "O(n log n)": int(linearithmic),
        "O(n²)": int(quadratic),
        "O(n³)": int(cubic),
        "O(2^n)": int(exponential)
    }

# Compare complexities for different input sizes
sizes = [1, 10, 100, 1000]
print("Big O Complexity Comparison")
print("n    | O(1) | O(log n) | O(n)   | O(n log n) | O(n²)     | O(n³)       | O(2^n)")
print("-" * 80)

for n in sizes:
    complexities = compare_complexities(n)
    if n <= 20:  # Only show exponential for small n
        print(f"{n:3} | {complexities['O(1)']:4} | {complexities['O(log n)']:8} | {complexities['O(n)']:6} | {complexities['O(n log n)']:10} | {complexities['O(n²)']:9} | {complexities['O(n³)']:11} | {complexities['O(2^n)']:,}")
    else:
        print(f"{n:3} | {complexities['O(1)']:4} | {complexities['O(log n)']:8} | {complexities['O(n)']:6} | {complexities['O(n log n)']:10} | {complexities['O(n²)']:9} | {complexities['O(n³)']:11} | {'∞':>7}")
```

```warning title="Exponential Growth is Dangerous"
Notice how exponential algorithms ($O(2^n)$) become impossible even for modest input sizes. An algorithm that's $O(2^n)$ can't handle n=30 on any computer - it would take billions of years!
```

## Visual Understanding of Growth Rates

Let's create a visual representation of how these complexities grow:

```plot
type: line
title: Algorithm Complexity Growth Comparison (Small Scale)
data:
  - name: "O(1) - Constant"
    x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
    y: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  - name: "O(log n) - Logarithmic" 
    x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
    y: [1, 1, 1.58, 2, 2.32, 2.58, 2.81, 3, 3.17, 3.32, 3.46, 3.58, 3.7, 3.81, 3.91, 4, 4.09, 4.17, 4.25, 4.32]
  - name: "O(n) - Linear"
    x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
    y: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
  - name: "O(n^2) - Quadratic"
    x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
    y: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400]
options:
  xLabel: "Input Size (n)"
  yLabel: "Operations"
  interactive: true
```

```plot
type: line
title: Algorithm Complexity Growth Comparison (Large Scale)
data:
  - name: "O(log n) - Logarithmic"
    x: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]
    y: [6.64, 7.64, 8.23, 8.64, 8.97, 9.23, 9.46, 9.64, 9.81, 9.97, 10.11, 10.23, 10.35, 10.46, 10.55, 10.64, 10.73, 10.81, 10.89, 10.97]
  - name: "O(n) - Linear"
    x: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]
    y: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]
  - name: "O(n log n) - Linearithmic"
    x: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]
    y: [664, 1529, 2468, 3456, 4485, 5548, 6621, 7712, 8824, 9966, 11122, 11292, 13496, 14623, 15774, 16935, 18109, 19296, 20501, 21931]
  - name: "O(n^2) - Quadratic"
    x: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]
    y: [10000, 40000, 90000, 160000, 250000, 360000, 490000, 640000, 810000, 1000000, 1210000, 1440000, 1690000, 1960000, 2250000, 2560000, 2890000, 3240000, 3610000, 4000000]
options:
  xLabel: "Input Size (n)"
  yLabel: "Operations"
  interactive: true
```

## Formal Mathematical Definitions

While we've been using Big O notation intuitively, let's understand the precise mathematical definitions behind asymptotic notation.

### Big O Notation (Upper Bound)

**Definition**: A function $f(n)$ is $O(g(n))$ if there exist positive constants $c$ and $n_0$ such that:

$$f(n) \leq c \cdot g(n) \text{ for all } n \geq n_0$$

This means $g(n)$ is an **upper bound** for $f(n)$ - the function $f(n)$ will never grow faster than some constant multiple of $g(n)$ for sufficiently large inputs.

```python-execute
def demonstrate_big_o_definition():
    """Demonstrate the mathematical definition of Big O"""
    
    # Example: f(n) = 2n + 10 is O(n)
    # We need to find c and n_0 such that 2n + 10 ≤ c·n for all n ≥ n_0
    
    def f(n):
        return 2*n + 10
    
    def g(n):
        return n
    
    # Let's try c = 3 and n_0 = 10
    c = 3
    n_0 = 10
    
    print("Big O Definition Example: f(n) = 2n + 10 is O(n)")
    print(f"We claim: f(n) ≤ {c} · g(n) for all n ≥ {n_0}")
    print()
    print("Verification:")
    print("n    | f(n) = 2n+10 | c·g(n) = 3n | f(n) ≤ c·g(n)?")
    print("-" * 50)
    
    for n in [5, 10, 15, 20, 25, 30, 50, 100]:
        f_n = f(n)
        cg_n = c * g(n)
        valid = "✓" if f_n <= cg_n else "✗"
        satisfies_n0 = n >= n_0
        
        print(f"{n:3} | {f_n:12} | {cg_n:11} | {valid:10} {'(n≥n₀)' if satisfies_n0 else '(n<n₀)'}")
    
    print(f"\nFor n ≥ {n_0}, we have f(n) ≤ {c}·g(n), so f(n) = O(n) ✓")

demonstrate_big_o_definition()
```

```plot
type: line
title: "Big O Definition Visualization: f(n) = 2n + 10 is O(n)"
data:
  - name: "f(n) = 2n + 10"
    x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
    y: [12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70]
  - name: "c·g(n) = 3n (upper bound)"
    x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
    y: [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90]
  - name: "n0 = 10 threshold"
    x: [10, 10]
    y: [0, 90]
options:
  xLabel: "Input Size (n)"
  yLabel: "Function Value"
  interactive: true
```

### Omega Notation (Lower Bound)

**Definition**: A function $f(n)$ is $\Omega(g(n))$ if there exist positive constants $c$ and $n_0$ such that:

$$f(n) \geq c \cdot g(n) \text{ for all } n \geq n_0$$

This means $g(n)$ is a **lower bound** for $f(n)$ - the function $f(n)$ will always grow at least as fast as some constant multiple of $g(n)$.

```python-execute
def demonstrate_omega_notation():
    """Demonstrate Omega (lower bound) notation"""
    
    # Example: f(n) = n² + 2n is Ω(n²)
    # We need to find c and n_0 such that n² + 2n ≥ c·n² for all n ≥ n_0
    
    def f(n):
        return n*n + 2*n
    
    def g(n):
        return n*n
    
    # Let's try c = 1 and n_0 = 1
    c = 1
    n_0 = 1
    
    print("Omega Notation Example: f(n) = n² + 2n is Ω(n²)")
    print(f"We claim: f(n) ≥ {c} · g(n) for all n ≥ {n_0}")
    print()
    print("Verification:")
    print("n    | f(n) = n²+2n | c·g(n) = n² | f(n) ≥ c·g(n)?")
    print("-" * 50)
    
    for n in [1, 2, 5, 10, 20, 50]:
        f_n = f(n)
        cg_n = c * g(n)
        valid = "✓" if f_n >= cg_n else "✗"
        
        print(f"{n:3} | {f_n:12} | {cg_n:11} | {valid:10}")
    
    print(f"\nSince n² + 2n ≥ n² for all n ≥ 1, we have f(n) = Ω(n²) ✓")

demonstrate_omega_notation()
```

```plot
type: line
title: "Omega Notation Visualization: f(n) = n^2 + 2n is Ω(n^2)"
data:
  - name: "f(n) = n^2 + 2n"
    x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
    y: [3, 8, 15, 24, 35, 48, 63, 80, 99, 120, 143, 168, 195, 224, 255, 288, 323, 360, 399, 440]
  - name: "c·g(n) = n^2 (lower bound)"
    x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
    y: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400]
options:
  xLabel: "Input Size (n)"
  yLabel: "Function Value"
  interactive: true
```

### Theta Notation (Tight Bound)

**Definition**: A function $f(n)$ is $\Theta(g(n))$ if and only if:
- $f(n) = O(g(n))$ AND $f(n) = \Omega(g(n))$

Equivalently, there exist positive constants $c_1$, $c_2$, and $n_0$ such that:

$$c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \text{ for all } n \geq n_0$$

This means $g(n)$ is a **tight bound** - $f(n)$ and $g(n)$ have the same growth rate.

```python-execute
def demonstrate_theta_notation():
    """Demonstrate Theta (tight bound) notation"""
    
    # Example: f(n) = 2n² + 3n + 1 is Θ(n²)
    # We need c₁ and c₂ such that c₁·n² ≤ 2n² + 3n + 1 ≤ c₂·n² for large n
    
    def f(n):
        return 2*n*n + 3*n + 1
    
    def g(n):
        return n*n
    
    # Let's try c₁ = 1, c₂ = 3, n₀ = 10
    c1, c2, n_0 = 1, 3, 10
    
    print("Theta Notation Example: f(n) = 2n² + 3n + 1 is Θ(n²)")
    print(f"We claim: {c1}·n² ≤ f(n) ≤ {c2}·n² for all n ≥ {n_0}")
    print()
    print("Verification:")
    print("n    | c₁·n² | f(n) = 2n²+3n+1 | c₂·n² | Lower ≤ f ≤ Upper?")
    print("-" * 65)
    
    for n in [5, 10, 15, 20, 30, 50, 100]:
        lower = c1 * g(n)
        f_n = f(n)
        upper = c2 * g(n)
        valid = "✓" if lower <= f_n <= upper else "✗"
        
        print(f"{n:3} | {lower:5} | {f_n:15} | {upper:5} | {valid:10}")
    
    print(f"\nFor n ≥ {n_0}, we have {c1}·n² ≤ f(n) ≤ {c2}·n², so f(n) = Θ(n²) ✓")

demonstrate_theta_notation()
```

```plot
type: line
title: "Theta Notation Visualization: f(n) = 2n^2 + 3n + 1 is Θ(n^2)"
data:
  - name: "f(n) = 2n^2 + 3n + 1"
    x: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
    y: [66, 91, 120, 153, 190, 231, 276, 325, 378, 435, 496, 561, 630, 703, 780, 861, 946, 1035, 1128, 1225, 1326]
  - name: "c1·g(n) = 1·n^2 (lower bound)"
    x: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
    y: [25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625]
  - name: "c2·g(n) = 3·n^2 (upper bound)"
    x: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
    y: [75, 108, 147, 192, 243, 300, 363, 432, 507, 588, 675, 768, 867, 972, 1083, 1200, 1323, 1452, 1587, 1728, 1875]
  - name: "n0 = 10 threshold"
    x: [10, 10]
    y: [0, 1400]
options:
  xLabel: "Input Size (n)"
  yLabel: "Function Value"
  interactive: true
```

### Little o and Little Omega (Strict Bounds)

For completeness, let's briefly cover the strict bound notations:

**Little o Notation (o)**: A function f(n) is o(g(n)) if f(n) grows **strictly slower** than g(n). Formally:

$$\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$$

**Example**: $n$ is $o(n^2)$ because $n$ grows strictly slower than $n^2$. As $n$ gets large, $n/n^2 = 1/n$ approaches 0.

**Little omega Notation (ω)**: A function f(n) is ω(g(n)) if f(n) grows **strictly faster** than g(n). Formally:

$$\lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty$$

**Example**: $n^2$ is $\omega(n)$ because $n^2$ grows strictly faster than $n$. As $n$ gets large, $n^2/n = n$ approaches infinity.

```note title="Key Differences from Big O and Omega"
- **Big O (O)** allows equality: f(n) can grow at the same rate as g(n)
- **Little o (o)** forbids equality: f(n) must grow strictly slower than g(n)
- **Big Omega (Ω)** allows equality: f(n) can grow at the same rate as g(n)  
- **Little omega (ω)** forbids equality: f(n) must grow strictly faster than g(n)
```

### Practical Implications of Different Notations

```table
title: Asymptotic Notation Comparison
headers: ["Notation", "Symbol", "Meaning", "Use Case", "Example"]
rows:
  - ["Big O", "$O(g(n))$", "Upper bound ($\leq$)", "Worst-case analysis", "Bubble sort is $O(n^2)$"]
  - ["Big Omega", "$\Omega(g(n))$", "Lower bound ($\geq$)", "Best-case analysis", "Any comparison sort is $\Omega(n \log n)$"]  
  - ["Big Theta", "$\\Theta(g(n))$", "Tight bound (=)", "Average-case analysis", "Merge sort is $\\Theta(n \\log n)$"]
  - ["Little o", "$o(g(n))$", "Strict upper bound (<)", "Dominance relations", "$n$ is $o(n^2)$"]
  - ["Little omega", "$\omega(g(n))$", "Strict lower bound (>)", "Dominance relations", "$n^2$ is $\omega(n)$"]
sortable: true
```

```note title="Why Different Notations Matter"
- **Big O** tells us the algorithm won't perform worse than this bound
- **Big Omega** tells us the algorithm won't perform better than this bound  
- **Big Theta** tells us the algorithm's performance is essentially this function
- In practice, we often use Big O even when we mean Big Theta, but understanding the distinction is important for precise analysis
```

```quiz
id: asymptotic-notation-quiz
questions:
  - id: q1
    question: "If f(n) = 5n^2 + 3n + 7, which statement is correct?"
    options:
      - id: a
        text: "$f(n) = O(n)$ only"
        correct: false
      - id: b  
        text: '$f(n) = O(n^2)$ and $f(n) = \Omega(n^2)$, so $f(n) = \Theta(n^2)$'
        correct: true
      - id: c
        text: '$f(n) = \Omega(n^3)$'
        correct: false
      - id: d
        text: '$f(n) = O(n^3)$ but not $\Omega(n^2)$'
        correct: false
    explanation: Since the dominant term is $n^2$, the function is both $O(n^2)$ and $\Omega(n^2)$, making it $\Theta(n^2)$.
  
  - id: q2
    question: "What does it mean if an algorithm is $O(n^2)$ but $\\Omega(n)$?"
    options:
      - id: a
        text: The algorithm always takes exactly $n^2$ time
        correct: false
      - id: b
        text: The worst case is quadratic, best case is linear
        correct: true
      - id: c
        text: The algorithm is broken
        correct: false
      - id: d
        text: This is mathematically impossible
        correct: false
    explanation: This means the algorithm's performance varies from linear (best case) to quadratic (worst case) depending on input characteristics.
```

## Choosing Algorithms Based on Big O

Understanding Big O notation helps you make smart algorithm choices for real applications:

```python-execute
def compare_sorting_algorithms():
    """Compare sorting algorithms for a growing dataset"""
    
    # Simulate operations for different sorting algorithms
    sizes = [1000, 5000, 10000, 50000, 100000]
    
    print("Sorting Algorithm Performance Comparison")
    print("Size     | Bubble O(n²) | Selection O(n²) | Merge O(n log n)")
    print("-" * 60)
    
    for n in sizes:
        bubble_ops = n * n                    # O(n²) 
        selection_ops = n * n                 # O(n²)
        merge_ops = n * int(n.bit_length())   # O(n log n) approximation
        
        print(f"{n:5,} | {bubble_ops:10,} | {selection_ops:11,} | {merge_ops:13,}")
    
    print("\n🎯 Decision: For growing data (1K → 100K items), choose O(n log n)!")
    print("   At 100K items: O(n²) needs 10 billion ops, O(n log n) needs ~1.7 million ops")

compare_sorting_algorithms()
```

```quiz
id: algorithm-choice
question: "You need to sort integers in a list that will grow from 1,000 to 100,000 items over time. Which sorting algorithm should you choose?"
options:
  - id: a
    text: "Radix sort - $O(n)$"
    correct: true
    explanation: "Correct! $O(n)$ is the best complexity for large datasets. It grows linearly with input size."
  - id: b
    text: 'Merge sort - $O(n \log n)$'
    correct: false
    explanation: "While $O(n \\log n)$ is good, $O(n)$ is better for large datasets. At 100K items, $O(n)$ needs 100K operations vs $O(n \\log n)$ ~1.7 million."
  - id: c
    text: "Bubble sort - $O(n^2)$"
    correct: false
    explanation: "$O(n^2)$ becomes unusably slow for large data. At 100K items, you'd need 10 billion operations!"
  - id: d
    text: "Bogo sort - $O(n!)$"
    correct: false
    explanation: "$O(n!)$ is catastrophically slow! At just 10 items, you'd need over 3 million operations. This would never finish for 100K items."
```

```quiz
id: asymptotic-analysis-practice
title: Asymptotic Notation Practice
questions:
  - id: q1
    question: "What is the Big Theta $(\\Theta)$ classification of $f_1(n) = 4n^3 + 2n^2 - 5n + 10$?"
    options:
      - id: a
        text: '$\Theta(n^2)$'
        correct: false
        explanation: The $n^3$ term dominates for large n, not the $n^2$ term.
      - id: b
        text: '$\Theta(n^3)$'
        correct: true
        explanation: 'Correct! The highest-degree term $4n^3$ dominates, making it $\Theta(n^3)$.'
      - id: c
        text: '$\Theta(n^4)$'
        correct: false
        explanation: There is no $n^4$ term in this function.
      - id: d
        text: '$\Theta(4n^3)$'
        correct: false
        explanation: In Big Theta notation, we drop constant coefficients.
  
  - id: q2
    question: "What is the Big Theta classification of $f_2(n) = n \\log_2(n) + 100n$?"
    options:
      - id: a
        text: '$\Theta(n)$'
        correct: false
        explanation: 'The $n \log n$ term grows faster than linear n for large n.'
      - id: b
        text: '$\Theta(n \log n)$'
        correct: true
        explanation: 'Correct! Both terms are $O(n \log n)$, so the function is $\Theta(n \log n)$.'
      - id: c
        text: '$\Theta(\log n)$'
        correct: false
        explanation: 'The n terms make this much larger than just $\log n$.'
      - id: d
        text: '$\Theta(n^2)$'
        correct: false
        explanation: This function grows slower than quadratic.
  
  - id: q3
    question: "What is the **simplified** Big Theta classification of $f_3(n) = 2^n + n^3$?"
    options:
      - id: a
        text: '$\Theta(n^3)$'
        correct: false
        explanation: Exponential functions always dominate polynomial functions for large n.
      - id: b
        text: '$\Theta(2^n)$'
        correct: true
        explanation: Correct! We simplify to the dominant term. The exponential $2^n$ grows much faster than the polynomial $n^3$.
      - id: c
        text: '$\Theta(2^n + n^3)$'
        correct: false
        explanation: While technically valid, we conventionally simplify to the dominant term for cleaner analysis.
      - id: d
        text: '$\Theta(n^3 \cdot 2^n)$'
        correct: false
        explanation: The terms are added, not multiplied.
```

## Analyzing Simple Algorithms

Let's learn to determine Big O complexity by analyzing code structure:

### O(1) - Constant Time

```python-execute
def constant_time_examples():
    """Examples of O(1) operations"""
    
    # Array/list access by index
    def get_first_element(arr):
        if len(arr) > 0:
            return arr[0]  # Always 1 operation regardless of array size
        return None
    
    # Dictionary/hash table lookup
    def lookup_student_grade(grades, student_id):
        return grades.get(student_id)  # Hash lookup is O(1) average case
    
    # Mathematical operations
    def calculate_area(length, width):
        return length * width  # Always 1 multiplication
    
    # Test with different sizes
    small_list = [1, 2, 3]
    large_list = list(range(10000))
    
    print("O(1) - Constant Time Operations:")
    print("Operation takes same time regardless of input size")
    print(f"First element of 3-item list: {get_first_element(small_list)}")
    print(f"First element of 10,000-item list: {get_first_element(large_list)}")
    
    return "Both operations take exactly the same number of steps!"

result = constant_time_examples()
print(result)
```

### O(n) - Linear Time

```python-execute
def linear_time_examples():
    """Examples of O(n) algorithms"""
    
    def find_maximum(arr):
        """O(n) - must check every element once"""
        operations = 0
        if not arr:
            return None, operations
            
        max_val = arr[0]
        for val in arr:
            operations += 1
            if val > max_val:
                max_val = val
        return max_val, operations
    
    def print_all_elements(arr):
        """O(n) - must visit every element once"""
        operations = 0
        for element in arr:
            operations += 1
            # print(element)  # Commented to avoid excessive output
        return operations
    
    def linear_search(arr, target):
        """O(n) - might need to check every element"""
        operations = 0
        for i, val in enumerate(arr):
            operations += 1
            if val == target:
                return i, operations
        return -1, operations
    
    # Test with different sizes
    sizes = [100, 1000, 10000]
    print("O(n) - Linear Time Analysis:")
    print("Size   | Find Max | Print All | Linear Search (avg)")
    print("-" * 50)
    
    for size in sizes:
        test_data = list(range(size))
        
        _, max_ops = find_maximum(test_data)
        print_ops = print_all_elements(test_data)
        _, search_ops = linear_search(test_data, size // 2)  # Search for middle element
        
        print(f"{size:5} | {max_ops:8} | {print_ops:9} | {search_ops:13}")

linear_time_examples()
```

### $O(n^2)$ - Quadratic Time

```python-execute
def quadratic_time_examples():
    """Examples of O(n²) algorithms"""
    
    def bubble_sort_analysis(arr):
        """O(n²) - nested loops over the array"""
        operations = 0
        n = len(arr)
        arr = arr.copy()  # Don't modify original
        
        for i in range(n):
            for j in range(0, n - i - 1):
                operations += 1
                if arr[j] > arr[j + 1]:
                    arr[j], arr[j + 1] = arr[j + 1], arr[j]
        
        return arr, operations
    
    def find_all_pairs(arr):
        """O(n²) - check every pair of elements"""
        operations = 0
        pairs = []
        
        for i in range(len(arr)):
            for j in range(i + 1, len(arr)):
                operations += 1
                pairs.append((arr[i], arr[j]))
        
        return len(pairs), operations
    
    def matrix_multiplication_ops(n):
        """O(n³) for n×n matrices, but let's see n² pattern first"""
        # For n×n matrix, we have n² elements to compute
        # Each element requires n operations (in actual matrix mult)
        return n * n  # This shows the n² part
    
    # Test quadratic growth
    sizes = [10, 20, 50, 100]
    print("O(n²) - Quadratic Time Analysis:")
    print("Size | Bubble Sort | All Pairs | Matrix Elements")
    print("-" * 45)
    
    for size in sizes:
        test_data = list(range(size, 0, -1))  # Worst case: reverse sorted
        
        _, bubble_ops = bubble_sort_analysis(test_data)
        pair_count, pair_ops = find_all_pairs(list(range(size)))
        matrix_elements = matrix_multiplication_ops(size)
        
        print(f"{size:3} | {bubble_ops:11} | {pair_ops:9} | {matrix_elements:15}")

quadratic_time_examples()
```

```quiz
id: big-o-identification
question: "What is the Big O complexity of this code? `for i in range(n): for j in range(n): print(i, j)`"
options:
  - id: a
    text: "$O(n)$"
    correct: false
    explanation: "This has nested loops, so it's more than linear. Each value of i causes n iterations of the inner loop."
  - id: b
    text: "$O(n^2)$"
    correct: true
    explanation: "Correct! The nested loops create $n \times n = n^2$ total operations, making this $O(n^2)$."
  - id: c
    text: '$O(n \log n)$'
    correct: false
    explanation: '$O(n \log n)$ typically comes from divide-and-conquer algorithms, not nested loops.'
  - id: d
    text: "$O(1)$"
    correct: false
    explanation: "This would be constant only if the loops didn't depend on input size n."
```

## Big O Analysis Rules

Understanding these rules helps you analyze any algorithm:

### Rule 1: Drop Constants

```python-execute
def demonstrate_constant_dropping():
    """Show why constants don't matter in Big O"""
    
    def algorithm_a(n):
        """5n operations - still O(n)"""
        operations = 0
        for i in range(n):
            operations += 5  # 5 operations per iteration
        return operations
    
    def algorithm_b(n):
        """100n operations - still O(n)"""
        operations = 0
        for i in range(n):
            operations += 100  # 100 operations per iteration
        return operations
    
    print("Constants Don't Matter in Big O:")
    print("n     | Algorithm A (5n) | Algorithm B (100n) | Ratio")
    print("-" * 55)
    
    for n in [100, 1000, 10000, 100000]:
        a_ops = algorithm_a(n)
        b_ops = algorithm_b(n)
        ratio = b_ops / a_ops
        
        print(f"{n:5} | {a_ops:15} | {b_ops:17} | {ratio:.1f}x")
    
    print("\nBoth algorithms are O(n) because the ratio stays constant!")

demonstrate_constant_dropping()
```

### Rule 2: Drop Lower-Order Terms

```python-execute
def demonstrate_dominant_terms():
    """Show why lower-order terms don't matter"""
    
    def complex_algorithm(n):
        """n² + 100n + 50 operations - still O(n²)"""
        quadratic = n * n
        linear = 100 * n  
        constant = 50
        
        total = quadratic + linear + constant
        return quadratic, linear, constant, total
    
    print("Lower-Order Terms Become Irrelevant:")
    print("n     | n² term    | 100n term | 50 term | Total      | n² %")
    print("-" * 65)
    
    for n in [10, 100, 1000, 10000]:
        quad, lin, const, total = complex_algorithm(n)
        percentage = (quad / total) * 100
        
        print(f"{n:5} | {quad:10,} | {lin:9,} | {const:7} | {total:10,} | {percentage:4.1f}%")
    
    print("\nAs n grows, the n² term dominates - that's why it's O(n²)!")

demonstrate_dominant_terms()
```

### Rule 3: Analyze Worst Case

```python-execute
def demonstrate_worst_case():
    """Show why we analyze worst-case scenarios"""
    
    def linear_search_detailed(arr, target):
        """Returns position and comparisons for linear search"""
        for i, val in enumerate(arr):
            if val == target:
                return i, i + 1  # Found at position i, made i+1 comparisons
        return -1, len(arr)  # Not found, checked all elements
    
    test_data = list(range(1000))
    
    # Best case: target at beginning
    best_pos, best_ops = linear_search_detailed(test_data, 0)
    
    # Average case: target in middle
    avg_pos, avg_ops = linear_search_detailed(test_data, 500)
    
    # Worst case: target at end (or not found)
    worst_pos, worst_ops = linear_search_detailed(test_data, 999)
    
    print("Linear Search Analysis (1000 elements):")
    print(f"Best case (target at start):    {best_ops} comparisons")
    print(f"Average case (target in middle): {avg_ops} comparisons")
    print(f"Worst case (target at end):     {worst_ops} comparisons")
    print(f"\nBig O focuses on worst case: O(n) = {worst_ops} for n=1000")

demonstrate_worst_case()
```

## Analyzing Code Patterns

Let's practice identifying Big O complexity from common code patterns:

```python-execute
def analyze_code_patterns():
    """Practice identifying Big O from code structure"""
    
    patterns = [
        ("Single loop", "for i in range(n): print(i)", "O(n)"),
        ("Nested loops", "for i in range(n): for j in range(n): print(i,j)", "O(n²)"),
        ("Sequential loops", "for i in range(n): print(i)\nfor j in range(n): print(j)", "O(n)"),
        ("Constant work", "return arr[0] if arr else None", "O(1)"),
        ("Halving search", "while n > 1: n = n // 2", "O(log n)"),
        ("Triple nesting", "for i in range(n): for j in range(n): for k in range(n): pass", "O(n³)")
    ]
    
    print("Common Code Patterns and Their Big O:")
    print("-" * 50)
    
    for name, code, complexity in patterns:
        print(f"{name:15}: {complexity}")
        print(f"   Pattern: {code}")
        print()

analyze_code_patterns()
```

```quiz
id: big-o-analysis-practice
title: Big O Code Analysis
description: Analyze code sections to determine overall Big O complexity.

questions:
  - id: q1
    question: |
      Consider this function:
      ```python
      def mystery_function(n):
          total = 0
          # Section 1
          for i in range(n):
              total += i
          # Section 2  
          for i in range(n):
              for j in range(n):
                  if i == j:
                      total += 1
          # Section 3
          for i in range(10):
              total += i
          return total
      ```
      What is the overall Big O complexity?
    options:
      - id: a
        text: "$O(n)$"
        correct: false
        explanation: Section 2 has nested loops creating $O(n^2)$ complexity, which dominates the $O(n)$ from Section 1.
      - id: b
        text: "$O(n^2)$"
        correct: true
        explanation: Correct! Section 1 is $O(n)$, Section 2 is $O(n^2)$, Section 3 is $O(1)$. The highest complexity $O(n^2)$ dominates.
      - id: c
        text: "$O(1)$"
        correct: false
        explanation: While Section 3 is $O(1)$, the other sections depend on n, making the overall complexity larger.
      - id: d
        text: "$O(n^3)$"
        correct: false
        explanation: There are no triple-nested loops here. The deepest nesting is 2 levels in Section 2.
  
  - id: q2
    question: |
      What is the complexity of Section 2 alone?
      ```python
      for i in range(n):
          for j in range(n):
              if i == j:
                  total += 1
      ```
    options:
      - id: a
        text: "$O(n)$"
        correct: false
        explanation: This has nested loops - the inner loop runs $n$ times for each of the $n$ iterations of the outer loop.
      - id: b
        text: "$O(n^2)$"
        correct: true
        explanation: 'Correct! Nested loops where both iterate $n$ times create $n \times n = O(n^2)$ complexity.'
      - id: c
        text: "$O(1)$"
        correct: false
        explanation: The loops depend on $n$, so this is not constant time.
      - id: d
        text: '$O(\log n)$'
        correct: false
        explanation: Simple nested loops create quadratic complexity, not logarithmic.
```

```exercise
id: complexity-ranking-exercise
title: Rank Big O Complexities
description: Sort the given complexity functions from fastest-growing to slowest-growing and output the corresponding letters.
difficulty: hard
starterCode: |
  # Given complexity functions (DO NOT modify these comments):
  # A: O(n!)          - factorial
  # B: O(1)           - constant  
  # C: O(n²)          - quadratic
  # D: O(n log n)     - linearithmic
  # E: O(2ⁿ)          - exponential base 2
  # F: O(n)           - linear
  # G: O(log n)       - logarithmic
  # H: O(n³)          - cubic
  # I: O(3ⁿ)          - exponential base 3
  # J: O(√n)          - square root
  # K: O(n log log n) - n times log of log n
  # L: O(nⁿ)          - n to the power of n

  # Your task: analyze the growth rates and replace the string below
  # Sort from FASTEST (smallest) to SLOWEST (largest) growing
  # For example, if B grows fastest and A grows slowest, print "B...A"
  # Hint: Think about how these functions grow as n gets very large
  
  print("Replace this string with your answer")
testCases:
  - expectedOutput: "BGJFKDCHEIAL"
    hidden: true
hints:
  - "Constant $O(1)$ is always fastest regardless of input size"
  - "Logarithmic $O(\\log n)$ grows very slowly"
  - "Square root $O(\\sqrt{n})$ grows faster than log but slower than linear"
  - "Linear $O(n)$ grows proportionally with input"
  - "$O(n \\log \\log n)$ grows slightly faster than linear"
  - "$O(n \\log n)$ is between linear and quadratic"
  - "Polynomial functions: $O(n) < O(n^2) < O(n^3)$"
  - "For exponentials: $O(2^n) < O(3^n)$ since 3 > 2"
  - "Exponential functions grow much faster than any polynomial"
  - "Factorial $O(n!)$ grows extremely fast"
  - "$O(n^n)$ is the fastest growing - even faster than factorial!"
solution: |
  # Given complexity functions:
  # A: O(n!)          - factorial
  # B: O(1)           - constant  
  # C: O(n²)          - quadratic
  # D: O(n log n)     - linearithmic
  # E: O(2ⁿ)          - exponential base 2
  # F: O(n)           - linear
  # G: O(log n)       - logarithmic
  # H: O(n³)          - cubic
  # I: O(3ⁿ)          - exponential base 3
  # J: O(√n)          - square root
  # K: O(n log log n) - n times log of log n
  # L: O(nⁿ)          - n to the power of n

  def rank_complexities():
      """
      Sort the complexity functions from FASTEST (smallest) to SLOWEST (largest) growing.
      Growth rate order: $O(1) < O(\log n) < O(\sqrt{n}) < O(n) < O(n \log \log n) < O(n \log n) < O(n^2) < O(n^3) < O(2^n) < O(3^n) < O(n!) < O(n^n)$
      """
      # Mapping: B=O(1), G=O(log n), J=O(√n), F=O(n), K=O(n log log n), D=O(n log n), C=O(n²), H=O(n³), E=O(2ⁿ), I=O(3ⁿ), A=O(n!), L=O(nⁿ)
      print("BGJFKDCHEIAL")

  # Test the answer
  rank_complexities()
```

## Practical Big O Decision Making

Understanding Big O helps you make informed algorithmic choices:

```python-execute
def algorithm_choice_example():
    """Demonstrate choosing algorithms based on Big O analysis"""
    
    def small_dataset_scenario():
        print("Scenario 1: Small dataset (n < 100)")
        print("Algorithm A: O(n²) but simple implementation")
        print("Algorithm B: O(n log n) but complex setup")
        print("Choice: Algorithm A - simplicity matters for small n")
        print()
    
    def large_dataset_scenario():
        print("Scenario 2: Large dataset (n > 10,000)")
        print("Algorithm A: O(n²) - becomes unusable")
        print("Algorithm B: O(n log n) - scales well")
        print("Choice: Algorithm B - efficiency matters for large n")
        print()
    
    def growing_data_scenario():
        print("Scenario 3: Data size will grow over time")
        print("Algorithm A: O(n²) - will slow down dramatically")
        print("Algorithm B: O(n log n) - remains manageable")
        print("Choice: Algorithm B - plan for future growth")
        print()
    
    small_dataset_scenario()
    large_dataset_scenario()
    growing_data_scenario()
    
    # Demonstrate the math
    print("Performance Comparison at Different Scales:")
    print("n     | O(n²)      | O(n log n) | Difference")
    print("-" * 45)
    
    import math
    for n in [100, 1000, 10000, 100000]:
        quadratic = n * n
        linearithmic = int(n * math.log2(n))
        ratio = quadratic / linearithmic if linearithmic > 0 else float('inf')
        
        print(f"{n:5} | {quadratic:10,} | {linearithmic:10,} | {ratio:8.1f}x")

algorithm_choice_example()
```


## Key Takeaways

- **Big O describes growth rate**: How performance scales with input size, focusing on dominant terms
- **Common complexities matter**: $O(1)$, $O(\log n)$, $O(n)$, $O(n \log n)$, $O(n^2)$, $O(n^3)$, $O(2^n)$
- **Analysis rules**: Drop constants, drop lower-order terms, focus on worst case
- **Code patterns reveal complexity**: Single loops = $O(n)$, nested loops = $O(n^2)$, etc.
- **Practical decision-making**: Choose algorithms based on expected data size and growth
- **Prevention over optimization**: Better to choose the right algorithm initially

Big O notation gives you the power to predict algorithm performance and make informed choices. In the next section, we'll apply these concepts to analyze recursive algorithms, which have their own special patterns and considerations.

```note title="The Power of Mathematical Analysis"
With Big O notation, you can predict that an $O(n^2)$ algorithm will be 100x slower when data size increases 10x, without running a single experiment. This mathematical insight is what separates good programmers from great ones.
```