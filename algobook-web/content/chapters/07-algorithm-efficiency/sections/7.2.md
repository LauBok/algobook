# Measuring Performance: Timing and Counting Operations

In Section 7.1, you saw dramatic performance differences between algorithms. But how do we measure performance systematically? **Python gives us powerful tools to time code execution and count operations.** Let's learn how to measure algorithm performance like a professional.

## Learning Objectives

By the end of this section, you will:
- Use Python's `time` module to measure execution time
- Count fundamental operations in algorithms
- Conduct systematic performance experiments
- Understand the relationship between input size and runtime
- Apply profiling techniques to analyze code performance

## Python's Time Module

The most direct way to measure performance is timing how long code takes to run:

```python-execute
import time

def simple_timing_example():
    # Record start time
    start_time = time.time()
    
    # Do some work
    total = 0
    for i in range(1000000):
        total += i
    
    # Record end time
    end_time = time.time()
    
    # Calculate elapsed time
    elapsed_time = end_time - start_time
    print(f"Calculation took {elapsed_time:.4f} seconds")
    print(f"Result: {total}")

simple_timing_example()
```

```note title="Understanding time.time()"
`time.time()` returns the current time as a floating-point number of seconds since January 1, 1970. By subtracting start from end time, we get the elapsed duration in seconds.
```

Let's create a more robust timing function:

```python-execute
import time

def time_function(func, *args, **kwargs):
    """Time a function call and return the result and elapsed time"""
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    elapsed_time = end_time - start_time
    return result, elapsed_time

# Example: Time different summation methods
def sum_with_loop(n):
    total = 0
    for i in range(1, n + 1):
        total += i
    return total

def sum_with_formula(n):
    return n * (n + 1) // 2

def sum_with_builtin(n):
    return sum(range(1, n + 1))

# Test each method
n = 1000000
result1, time1 = time_function(sum_with_loop, n)
result2, time2 = time_function(sum_with_formula, n)
result3, time3 = time_function(sum_with_builtin, n)

print(f"Loop method:     {time1:.4f} seconds, result: {result1}")
print(f"Formula method:  {time2:.4f} seconds, result: {result2}")
print(f"Built-in method: {time3:.4f} seconds, result: {result3}")
```

```hint title="Multiple Runs for Accuracy"
Timing can vary due to system load and other factors. Professional performance testing runs algorithms multiple times and averages the results for more accurate measurements.
```

## Counting Operations

While timing shows real-world performance, counting fundamental operations helps us understand algorithm behavior more precisely:

```python-execute
def linear_search_with_count(data, target):
    """Linear search that counts operations"""
    comparisons = 0
    for i, value in enumerate(data):
        comparisons += 1  # Count each comparison
        if value == target:
            return i, comparisons
    return -1, comparisons

def binary_search_with_count(data, target):
    """Binary search that counts operations (assumes sorted data)"""
    comparisons = 0
    left, right = 0, len(data) - 1
    
    while left <= right:
        comparisons += 1  # Count each comparison
        mid = (left + right) // 2
        if data[mid] == target:
            return mid, comparisons
        elif data[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return -1, comparisons

# Compare operation counts
test_data = list(range(1000))  # Sorted list 0-999
target = 750

linear_result, linear_ops = linear_search_with_count(test_data, target)
binary_result, binary_ops = binary_search_with_count(test_data, target)

print(f"Linear search: found at index {linear_result} with {linear_ops} comparisons")
print(f"Binary search: found at index {binary_result} with {binary_ops} comparisons")
print(f"Binary search is {linear_ops / binary_ops:.1f}x more efficient")
```

```quiz
id: operation-counting
question: What is the main advantage of counting operations versus timing algorithms?
options:
  - id: a
    text: Counting is always more accurate than timing
    correct: false
    explanation: Both have their place. Timing shows real performance, while counting shows algorithmic behavior.
  - id: b
    text: Operation counts are independent of hardware and system load
    correct: true
    explanation: Correct! Operation counts reveal the inherent algorithmic complexity, while timing can vary based on hardware and system conditions.
  - id: c
    text: Counting is easier to implement
    correct: false
    explanation: Actually, adding counters can make code more complex, but it provides valuable insights.
  - id: d
    text: Counting works better for small datasets
    correct: false
    explanation: Both timing and counting work for any dataset size, but they measure different aspects.
```

## Systematic Performance Analysis

Let's conduct a systematic experiment to understand how algorithm performance scales with input size:

```python-execute
import time
import random

def performance_experiment():
    """Systematic performance analysis of different algorithms"""
    
    def linear_search_with_count(data, target):
        """Linear search that counts operations"""
        comparisons = 0
        for i, value in enumerate(data):
            comparisons += 1  # Count each comparison
            if value == target:
                return i, comparisons
        return -1, comparisons

    def binary_search_with_count(data, target):
        """Binary search that counts operations (assumes sorted data)"""
        comparisons = 0
        left, right = 0, len(data) - 1
        
        while left <= right:
            comparisons += 1  # Count each comparison
            mid = (left + right) // 2
            if data[mid] == target:
                return mid, comparisons
            elif data[mid] < target:
                left = mid + 1
            else:
                right = mid - 1
        
        return -1, comparisons
    
    # Test different input sizes
    sizes = [100, 500, 1000, 2000, 5000]
    
    print("Performance Analysis: Linear vs Binary Search")
    print("Size     | Linear (ms) | Binary (ms) | Linear Ops | Binary Ops")
    print("-" * 65)
    
    for size in sizes:
        # Create test data
        data = list(range(size))
        target = random.randint(size//4, 3*size//4)  # Target in middle range
        
        # Time linear search
        start = time.time()
        linear_result, linear_ops = linear_search_with_count(data, target)
        linear_time = (time.time() - start) * 1000  # Convert to milliseconds
        
        # Time binary search  
        start = time.time()
        binary_result, binary_ops = binary_search_with_count(data, target)
        binary_time = (time.time() - start) * 1000
        
        print(f"{size:4} | {linear_time:9.3f} | {binary_time:9.3f} | {linear_ops:8} | {binary_ops:8}")

performance_experiment()
```

## Understanding Performance Patterns

Let's visualize how performance scales with different input sizes:

```python-execute
def analyze_growth_patterns():
    """Analyze how different algorithms scale with input size"""
    
    sizes = [10, 50, 100, 500, 1000, 5000]
    
    print("Growth Pattern Analysis")
    print("Size   | Linear | Quadratic | Logarithmic | Constant")
    print("-" * 55)
    
    for size in sizes:
        linear_ops = size                    # O(n)
        quadratic_ops = size * size          # O(n²)
        logarithmic_ops = int(size.bit_length())  # Approximation of log₂(n)
        constant_ops = 1                     # O(1)
        
        print(f"{size:4} | {linear_ops:6} | {quadratic_ops:9} | {logarithmic_ops:11} | {constant_ops:8}")

analyze_growth_patterns()
```

```warning title="Quadratic Growth Warning"
Notice how quadratic operations (O(n²)) explode as input size grows! This is why nested loops can be dangerous for large datasets. An algorithm that's fine for 100 items might be unusable for 5000 items.
```

## Advanced Timing Techniques

For more precise measurements, Python provides additional timing tools:

```python-execute
import timeit

def advanced_timing_example():
    """Demonstrate more precise timing techniques"""
    
    # Using timeit for more accurate measurements
    setup_code = "data = list(range(1000)); target = 500"
    
    linear_code = """
for i, value in enumerate(data):
    if value == target:
        break
"""
    
    binary_code = """
left, right = 0, len(data) - 1
while left <= right:
    mid = (left + right) // 2
    if data[mid] == target:
        break
    elif data[mid] < target:
        left = mid + 1
    else:
        right = mid - 1
"""
    
    # Run each test multiple times for accuracy
    linear_time = timeit.timeit(linear_code, setup_code, number=10000)
    binary_time = timeit.timeit(binary_code, setup_code, number=10000)
    
    print("Precise timing with timeit (10,000 runs each):")
    print(f"Linear search:  {linear_time:.6f} seconds total, {linear_time/10000*1000:.6f} ms average")
    print(f"Binary search:  {binary_time:.6f} seconds total, {binary_time/10000*1000:.6f} ms average")
    print(f"Speed difference: {linear_time/binary_time:.1f}x")

advanced_timing_example()
```

## Memory Usage Analysis

Performance isn't just about time - memory usage matters too:

```python-execute
import sys

def memory_analysis():
    """Analyze memory usage of different data structures"""
    
    # List vs generator comparison
    size = 10000
    
    # Memory-intensive: Store all values
    memory_list = [x*x for x in range(size)]
    list_memory = sys.getsizeof(memory_list)
    
    # Memory-efficient: Generate values on demand
    memory_generator = (x*x for x in range(size))
    generator_memory = sys.getsizeof(memory_generator)
    
    print("Memory Usage Analysis:")
    print(f"List (stores all {size} values):      {list_memory:,} bytes")
    print(f"Generator (computes on demand):       {generator_memory:,} bytes")
    print(f"Memory savings: {list_memory/generator_memory:.1f}x less memory")
    
    # Show that generators still work
    generator_sum = sum(x*x for x in range(10))
    list_sum = sum([x*x for x in range(10)])
    print(f"Both produce same result: {generator_sum} = {list_sum}")

memory_analysis()
```

```note title="Time vs Space Tradeoffs"
Often there's a tradeoff between time and space. Generators use less memory but might compute values multiple times. Lists use more memory but provide instant access. Understanding these tradeoffs helps you choose the right approach.
```

## Practical Profiling

Let's put it all together with a comprehensive performance comparison:

```python-execute
import time

def comprehensive_performance_test():
    """Comprehensive test comparing multiple algorithms"""
    
    algorithms = {
        "Bubble Sort": lambda arr: bubble_sort_timed(arr.copy()),
        "Selection Sort": lambda arr: selection_sort_timed(arr.copy()), 
        "Python Sort": lambda arr: python_sort_timed(arr.copy())
    }
    
    sizes = [100, 500, 1000]
    
    print("Comprehensive Algorithm Performance Comparison")
    print("=" * 70)
    
    for size in sizes:
        print(f"\nTesting with {size} random numbers:")
        test_data = list(range(size, 0, -1))  # Worst case: reverse order
        
        for name, algorithm in algorithms.items():
            start_time = time.time()
            sorted_data, operations = algorithm(test_data)
            end_time = time.time()
            
            elapsed = (end_time - start_time) * 1000  # milliseconds
            print(f"  {name:12}: {elapsed:7.2f} ms, {operations:,} operations")

def bubble_sort_timed(arr):
    operations = 0
    n = len(arr)
    for i in range(n):
        for j in range(0, n-i-1):
            operations += 1
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr, operations

def selection_sort_timed(arr):
    operations = 0
    for i in range(len(arr)):
        min_idx = i
        for j in range(i+1, len(arr)):
            operations += 1
            if arr[j] < arr[min_idx]:
                min_idx = j
        arr[i], arr[min_idx] = arr[min_idx], arr[i]
    return arr, operations

def python_sort_timed(arr):
    # Python's sort is highly optimized, estimating operations
    operations = len(arr) * 15  # Rough estimate for comparison
    arr.sort()
    return arr, operations

comprehensive_performance_test()
```

```exercise
id: operation-counting-exercise
title: Operation Counting Practice
description: Add operation counting to the provided search algorithms. Focus on counting comparison operations to measure algorithmic efficiency.
difficulty: medium
starterCode: |
  def linear_search_counter(data, target):
      """Linear search that counts and returns operations performed"""
      operations = 0
      for i, value in enumerate(data):
          # Add operation counting here
          if value == target:
              return i, operations
      return -1, operations
  
  def binary_search_counter(data, target):
      """Binary search that counts and returns operations performed"""  
      operations = 0
      left, right = 0, len(data) - 1
      
      while left <= right:
          # Add operation counting here
          mid = (left + right) // 2
          if data[mid] == target:
              return mid, operations
          elif data[mid] < target:
              left = mid + 1
          else:
              right = mid - 1
      
      return -1, operations
  
  def analyze_search_efficiency(data, target):
      """Compare search algorithms by counting operations"""
      linear_index, linear_ops = linear_search_counter(data, target)
      binary_index, binary_ops = binary_search_counter(data, target)
      
      # Add print statement here to show the comparison
      # Format: "Linear: X operations, Binary: Y operations"
  
  # Test the functions
  test_data = list(range(100))  # Sorted list 0-99
  target = 75
  analyze_search_efficiency(test_data, target)
testCases:
  - input: "list(range(100)), 75"
    expectedOutput: "Linear: 76 operations, Binary: 4 operations"
  - input: "list(range(1000)), 500" 
    expectedOutput: "Linear: 501 operations, Binary: 10 operations"
hints:
  - "Add 'operations += 1' before each comparison (==, <, >, etc.)"
  - "In linear search, count before 'if value == target'"
  - "In binary search, count before 'if data[mid] == target'"
  - "Add the print statement in analyze_search_efficiency function"
solution: |
  def linear_search_counter(data, target):
      """Linear search that counts and returns operations performed"""
      operations = 0
      for i, value in enumerate(data):
          operations += 1  # Add operation counting here
          if value == target:
              return i, operations
      return -1, operations
  
  def binary_search_counter(data, target):
      """Binary search that counts and returns operations performed"""  
      operations = 0
      left, right = 0, len(data) - 1
      
      while left <= right:
          operations += 1  # Add operation counting here
          mid = (left + right) // 2
          if data[mid] == target:
              return mid, operations
          elif data[mid] < target:
              left = mid + 1
          else:
              right = mid - 1
      
      return -1, operations
  
  def analyze_search_efficiency(data, target):
      """Compare search algorithms by counting operations"""
      linear_index, linear_ops = linear_search_counter(data, target)
      binary_index, binary_ops = binary_search_counter(data, target)
      
      print(f"Linear: {linear_ops} operations, Binary: {binary_ops} operations")
  
  # Test the functions
  test_data = list(range(100))  # Sorted list 0-99
  target = 75
  analyze_search_efficiency(test_data, target)
```

## Key Takeaways

- **Timing reveals real-world performance**: Use `time.time()` and `timeit` for accurate measurements
- **Operation counting shows algorithmic behavior**: Independent of hardware, shows inherent complexity
- **Systematic testing is crucial**: Test multiple input sizes to understand scaling behavior
- **Multiple measurement approaches**: Combine timing, operation counting, and memory analysis
- **Hardware matters for timing**: Operation counts are more predictable across different systems
- **Professional profiling**: Use proper statistical methods (multiple runs, averages) for accuracy

Now that you can measure algorithm performance systematically, you're ready to learn the mathematical tools that predict performance without running experiments. Let's dive into Big O notation!

```quiz
id: measurement-techniques
question: You want to compare two algorithms that will run on different computers. Which measurement technique gives the most consistent results across different hardware?
options:
  - id: a
    text: Timing with time.time()
    correct: false
    explanation: Timing varies significantly across different hardware and system loads.
  - id: b
    text: Counting fundamental operations
    correct: true
    explanation: Correct! Operation counts reveal algorithmic complexity independent of hardware speed or system conditions.
  - id: c
    text: Measuring memory usage
    correct: false
    explanation: While useful, memory usage doesn't directly indicate algorithmic efficiency across different systems.
  - id: d
    text: Using timeit module
    correct: false
    explanation: While timeit is more accurate than basic timing, it still varies across different hardware.
```