# Performance Measurement

While Section 7.1 demonstrated performance differences conceptually, this section develops practical tools for quantifying algorithmic behavior. The most natural way to evaluate performance is by timing execution—measuring how long algorithms actually take to run. However, as we will see, timing measurements depend heavily on hardware, system load, and environmental factors, making it difficult to understand the inherent properties of algorithms themselves. We therefore also introduce operation counting as a hardware-independent proxy for performance, which leads naturally to mathematical analysis of scaling behavior.

## Learning Objectives

By the end of this section, you will:
- Use Python's timing modules to measure execution time
- Count fundamental operations as a proxy for performance
- Understand the limitations of different measurement approaches
- Conduct systematic performance experiments

## Real-World Timing

Performance measurement begins with timing actual execution. This approach directly reflects user experience: when a user initiates an operation, how long must they wait for completion?

Python's `time` module provides basic timing functionality:

```python-execute
import time

def simple_timing_example():
    start_time = time.time()

    # Computational work
    total = 0
    for i in range(1000000):
        total += i

    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Calculation took {elapsed_time:.4f} seconds")
    print(f"Result: {total}")

simple_timing_example()
```

```note title="Timing Resolution"
`time.time()` returns seconds since January 1, 1970 as a floating-point number. Subtracting start from end time yields the elapsed duration.
```

A systematic comparison of three summation approaches demonstrates timing utility:

```python-execute
import time

def time_algorithm(func, n):
    start_time = time.time()
    result = func(n)
    end_time = time.time()
    return result, end_time - start_time

def sum_with_loop(n):
    total = 0
    for i in range(1, n + 1):
        total += i
    return total

def sum_with_formula(n):
    return n * (n + 1) // 2

def sum_with_builtin(n):
    return sum(range(1, n + 1))

# Compare approaches
n = 1000000
result1, time1 = time_algorithm(sum_with_loop, n)
result2, time2 = time_algorithm(sum_with_formula, n)
result3, time3 = time_algorithm(sum_with_builtin, n)

print(f"Loop method:     {time1:.4f} seconds")
print(f"Formula method:  {time2:.4f} seconds")
print(f"Built-in method: {time3:.4f} seconds")
```

The `time` method only measures once and can be random due to system load and background processes. The `timeit` module reduces this variability by running code multiple times and averaging the results:

```python-execute
import timeit

def advanced_timing_comparison():
    data = list(range(1000))
    target = 500

    def linear_search():
        for i, value in enumerate(data):
            if value == target:
                break

    def binary_search():
        left, right = 0, len(data) - 1
        while left <= right:
            mid = (left + right) // 2
            if data[mid] == target:
                break
            elif data[mid] < target:
                left = mid + 1
            else:
                right = mid - 1

    linear_time = timeit.timeit(linear_search, number=10000)
    binary_time = timeit.timeit(binary_search, number=10000)

    print(f"Linear search:  {linear_time:.6f} seconds (10,000 runs)")
    print(f"Binary search:  {binary_time:.6f} seconds (10,000 runs)")
    print(f"Performance ratio: {linear_time/binary_time:.1f}x")

advanced_timing_comparison()
```

```note title="Timing Limitations"
Timing measurements depend on hardware performance, system load, and environmental factors. Results vary across different computers and execution contexts.
```

## Operation Counting as Performance Proxy

Operation counting avoids these timing issues by measuring algorithmic work rather than execution time. By counting fundamental operations—comparisons, assignments, arithmetic operations—we obtain measurements that remain identical regardless of hardware speed, system load, or environmental factors.

The following example demonstrates how operation counts predict relative performance differences:

```python-execute
def linear_search_with_count(data, target):
    comparisons = 0
    for i, value in enumerate(data):
        comparisons += 1  # Count each comparison
        if value == target:
            return i, comparisons
    return -1, comparisons

def binary_search_with_count(data, target):
    comparisons = 0
    left, right = 0, len(data) - 1

    while left <= right:
        comparisons += 1  # Count each comparison
        mid = (left + right) // 2
        if data[mid] == target:
            return mid, comparisons
        elif data[mid] < target:
            left = mid + 1
        else:
            right = mid - 1

    return -1, comparisons

# Demonstrate correlation between timing and operation counting
import time
import random

sizes = [100, 1000, 10000, 100000]
print("Size     | Linear Time | Binary Time | Linear Ops | Binary Ops")
print("-" * 65)

for size in sizes:
    data = list(range(size))
    target = random.randint(size//4, 3*size//4)

    # Time both algorithms
    start = time.time()
    linear_result, linear_ops = linear_search_with_count(data, target)
    linear_time = (time.time() - start) * 1000

    start = time.time()
    binary_result, binary_ops = binary_search_with_count(data, target)
    binary_time = (time.time() - start) * 1000

    print(f"{size:4} | {linear_time:9.3f} | {binary_time:9.3f} | {linear_ops:8} | {binary_ops:8}")
```

The table shows that operation counts provide consistent algorithm comparison without the noise of timing measurements. While absolute timing varies between runs, operation counts remain stable and often reflect how much real time will be needed.

```quiz
id: operation-counting-advantage
question: "What is the primary advantage of counting operations compared to timing algorithms?"
options:
  - id: a
    text: "Operation counts are always more accurate"
    correct: false
    explanation: "Both approaches provide useful information. Timing shows real performance while counting shows algorithmic complexity."
  - id: b
    text: "Operation counts are independent of hardware and system conditions"
    correct: true
    explanation: "Correct! Operation counts reveal algorithmic behavior regardless of hardware speed or system load."
  - id: c
    text: "Counting operations is easier to implement"
    correct: false
    explanation: "Adding operation counters requires code modification and careful tracking of fundamental operations."
  - id: d
    text: "Operation counting works better for small datasets"
    correct: false
    explanation: "Both timing and counting apply to any dataset size, measuring different aspects of performance."
```

### Discovering Growth Patterns

Operation counting reveals algorithmic growth characteristics. The following algorithms demonstrate different scaling behaviors:

```python-execute
def mystery_algorithm_alpha(n):
    operations = 0
    for i in range(n):
        operations += 1
    return operations

def mystery_algorithm_beta(n):
    operations = 0
    for i in range(n):
        for j in range(n):
            operations += 1
    return operations

def mystery_algorithm_gamma(n):
    operations = 0
    while n > 1:
        operations += 1
        n = n // 2
    return operations

# Analyze growth characteristics
print("Input Size | Linear (α) | Quadratic (β) | Logarithmic (γ)")
print("-" * 55)

for size in [10, 20, 40, 80, 160]:
    alpha_ops = mystery_algorithm_alpha(size)
    beta_ops = mystery_algorithm_beta(size)
    gamma_ops = mystery_algorithm_gamma(size)

    print(f"{size:8} | {alpha_ops:8} | {beta_ops:11} | {gamma_ops:13}")
```

```note title="Growth Classification"
- **Algorithm α**: Operations equal input size (linear growth)
- **Algorithm β**: Operations equal input size squared (quadratic growth)
- **Algorithm γ**: Operations equal logarithm of input size (logarithmic growth)
</note>

These growth patterns have dramatically different practical implications. An algorithm that works acceptably for small inputs may become unusable as data grows, depending entirely on its growth pattern. Linear algorithms remain manageable as data increases, quadratic algorithms become prohibitively slow, and logarithmic algorithms barely slow down at all.

Understanding these patterns explains why some algorithms scale to real-world problem sizes while others fail. A quadratic algorithm processing 1,000 items might be acceptable, but the same algorithm processing 1 million items becomes impractical regardless of hardware improvements.

### Focus on Dominant Operations

The growth rate when input size increases is more important than the exact number of operations. Just as real timing depends on hardware and system factors, exact operation counts depend on implementation details like how each line of code maps to CPU instructions and which operations we choose to count. Therefore, we focus on growth rates that capture the essential algorithmic behavior. For example, we may focus on representative operations that drive the growth rate, such as loop iterations or key operations, but do not try to accurately count each addition, subtraction, or assignment.

```exercise
id: operation-counting-exercise
title: Add Operation Counting to Search Algorithms
description: |
  Modify the provided search algorithms to count comparison operations. Add `operations += 1`
  statements before each comparison to measure algorithmic work independent of hardware performance.
difficulty: medium
prepend: |
  sizes_and_targets = input().strip().split()
  test_cases = []

  for i in range(0, len(sizes_and_targets), 2):
      size = int(sizes_and_targets[i])
      target = int(sizes_and_targets[i + 1])
      data = list(range(size))
      test_cases.append((data, target))
starterCode: |
  def linear_search_counter(data, target):
      operations = 0
      for i, value in enumerate(data):
          # TODO: Add operation counting before comparison
          if value == target:
              return i, operations
      return -1, operations

  def binary_search_counter(data, target):
      operations = 0
      left, right = 0, len(data) - 1

      while left <= right:
          # TODO: Add operation counting before comparison
          mid = (left + right) // 2
          if data[mid] == target:
              return mid, operations
          elif data[mid] < target:
              left = mid + 1
          else:
              right = mid - 1

      return -1, operations
postpend: |
  print("Operation Counting Results:")
  print("Size     | Linear Ops | Binary Ops | Efficiency Ratio")
  print("-" * 50)

  for data, target in test_cases:
      linear_index, linear_ops = linear_search_counter(data, target)
      binary_index, binary_ops = binary_search_counter(data, target)

      if binary_ops > 0:
          ratio = linear_ops / binary_ops
          print(f"{len(data):4} | {linear_ops:8} | {binary_ops:8} | {ratio:11.1f}x")
      else:
          print(f"{len(data):4} | {linear_ops:8} | {binary_ops:8} | {'N/A':>11}")
testCases:
  - input: "10 7 100 75 1000 500 10000 7500"
    expectedOutput: "Operation Counting Results:\\nSize     | Linear Ops | Binary Ops | Efficiency Ratio\\n--------------------------------------------------\\n  10 |        8 |        2 |         4.0x\\n 100 |       76 |        6 |        12.7x\\n1000 |      501 |        9 |        55.7x\\n10000 |     7501 |       13 |       577.0x"
hints:
  - "Add 'operations += 1' before each comparison operation"
  - "Count before checking 'if value == target' in linear search"
  - "Count before checking 'if data[mid] == target' in binary search"
solution: |
  def linear_search_counter(data, target):
      operations = 0
      for i, value in enumerate(data):
          operations += 1
          if value == target:
              return i, operations
      return -1, operations

  def binary_search_counter(data, target):
      operations = 0
      left, right = 0, len(data) - 1

      while left <= right:
          operations += 1
          mid = (left + right) // 2
          if data[mid] == target:
              return mid, operations
          elif data[mid] < target:
              left = mid + 1
          else:
              right = mid - 1

      return -1, operations
```

## Key Takeaways

- Timing provides intuitive performance measurement but depends on hardware and system conditions
- Operation counting eliminates hardware dependence by measuring algorithmic work directly
- Growth patterns (linear, quadratic, logarithmic) have dramatically different scalability implications
- Focusing on representative operations reveals growth rates more effectively than counting all operations
- Mathematical analysis of growth rates enables algorithm comparison without experimental measurement

Growth rates characterize how algorithm performance scales with input size—whether linearly, quadratically, logarithmically, or according to other patterns. The next section develops Big O notation, providing mathematical precision for analyzing and comparing these growth characteristics.