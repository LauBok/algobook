# Analyzing Recursive Algorithms

Section 7.3 established Big O notation for analyzing iterative algorithms through code pattern recognition. However, recursive algorithms present a fundamentally different challenge: the computational work is distributed across multiple recursive calls whose total contribution depends on complex mathematical relationships. Section 7.4 develops recurrence relations and the Master Theorem as systematic tools for analyzing recursive algorithm complexity.

Recursive algorithms solve problems by decomposing them into smaller subproblems of the same type. This decomposition creates mathematical relationships between the work performed at different problem sizes, requiring specialized analytical techniques beyond simple code inspection.

## Learning Objectives

By the end of this section, you will:
- Formulate recurrence relations from recursive algorithm structure
- Solve recurrence relations to determine asymptotic complexity
- Apply the Master Theorem to analyze divide-and-conquer algorithms
- Compare recursive and iterative approaches systematically
- Make informed decisions about recursive algorithm implementation

## Recurrence Relations Fundamentals

### Mathematical Foundation

A recurrence relation expresses the time complexity $T(n)$ of a recursive algorithm in terms of its complexity on smaller inputs. The general form captures the essential structure of recursive computation:

$$
T(n) = \begin{cases}
c & \text{if } n \leq n_0 \text{ (base case)} \\
\text{work at current level} + \sum \text{recursive calls on smaller inputs} & \text{if } n > n_0
\end{cases}
$$

Where:
- $T(n)$ represents the time complexity for input size $n$
- $c$ represents constant time for base cases
- $n_0$ defines the base case threshold

This mathematical formulation enables systematic analysis of recursive algorithm performance across different decomposition strategies.

### Linear Recurrence Analysis

Linear recurrences arise from algorithms that make a single recursive call on a reduced problem size. Consider the recursive factorial implementation:

```python-execute
def factorial_analysis(n, depth=0):
    """Factorial with operation counting"""
    indent = "  " * depth
    print(f"{indent}factorial({n}) called")

    if n <= 1:
        print(f"{indent}Base case: constant work")
        return 1, 1  # result, operations
    else:
        print(f"{indent}Recursive case: constant work + factorial({n-1})")
        result, ops = factorial_analysis(n - 1, depth + 1)
        return n * result, ops + 1

print("Factorial recurrence analysis:")
result, total_ops = factorial_analysis(5)
print(f"Total operations: {total_ops}")
```

**Recurrence relation setup:**
- Base case: $T(1) = c_1$ (constant work)
- Recursive case: $T(n) = T(n-1) + c_n$ (constant work plus smaller problem)

**Mathematical solution by expansion:**
$$
\begin{align}
T(n) &= T(n-1) + c_n \\
&= (T(n-2) + c_{n-1}) + c_n \\
&= T(1) + c_2 + c_3 + \cdots + c_n \\
&= \sum_{i=1}^{n} c_i
\end{align}
$$

Since the work at each level is $O(1)$, each $c_i \leq c$ for some constant $c$. Hence,
$$
T(n) \leq nc = O(n).
$$

This demonstrates that linear recurrences typically yield $O(n)$ complexity.

### Binary Recurrence Analysis

Binary recurrences occur when algorithms make two recursive calls, as in the naive Fibonacci implementation:

```python-execute
def fibonacci_analysis(n, depth=0):
    """Fibonacci with call counting"""
    indent = "  " * depth
    print(f"{indent}fib({n}) called")

    if n <= 1:
        print(f"{indent}Base case: return {n}")
        return n, 1  # result, calls
    else:
        print(f"{indent}Recursive case: fib({n-1}) + fib({n-2})")
        result1, calls1 = fibonacci_analysis(n - 1, depth + 1)
        result2, calls2 = fibonacci_analysis(n - 2, depth + 1)
        return result1 + result2, calls1 + calls2 + 1

print("Fibonacci recurrence analysis:")
if True:  # Set to False to avoid excessive output
    print("Analysis for small values:")
    for n in range(1, 6):
        result, calls = fibonacci_analysis(n)
        print(f"fib({n}): {calls} total calls")
        print()
```

**Recurrence relation:**
$$
T(n) = \begin{cases}
c & \text{if } n \leq 1 \\
T(n-1) + T(n-2) + c & \text{if } n > 1
\end{cases}
$$

**Mathematical analysis:**
This recurrence has the characteristic equation $r^2 - r - 1 = 0$, yielding roots:
$$
r = \frac{1 \pm \sqrt{5}}{2}.
$$

Let $\phi = (1 + \sqrt{5})/2 \approx 1.618$ (golden ratio). The solution is:
$$
T(n) = \Theta(\phi^n).
$$

This exponential growth explains why naive Fibonacci becomes impractical for moderate input sizes.

```table
title: "Binary vs Linear Recurrence Growth"
headers: ["$n$", "Linear $T(n) = O(n)$", "Binary $T(n) = O(\\phi^n)$", "Growth Ratio"]
rows:
  - ["5", "5", "15", "3x"]
  - ["10", "10", "177", "18x"]
  - ["15", "15", "1,973", "132x"]
  - ["20", "20", "21,891", "1,095x"]
  - ["25", "25", "242,785", "9,711x"]
caption: "Binary recurrences exhibit exponential growth"
sortable: false
```

### Divide-and-Conquer Recurrences

The most important class of recursive algorithms follows the divide-and-conquer paradigm:

1. **Divide** the problem into $a$ subproblems
2. **Conquer** by solving each subproblem of size $n/b$
3. **Combine** the results with $f(n)$ additional work

This yields the recurrence pattern: $T(n) = aT(n/b) + f(n)$

**Binary Search Example:**
From Chapter 6.4, binary search finds an element in a sorted array by repeatedly checking the middle element and eliminating half the search space:

```python-execute
def binary_search_recurrence_demo(arr, target, left=0, right=None, depth=0):
    """Binary search with recurrence visualization"""
    if right is None:
        right = len(arr) - 1

    indent = "  " * depth
    print(f"{indent}Searching in range [{left}, {right}], size = {right - left + 1}")

    if left > right:
        print(f"{indent}Base case: not found")
        return -1

    mid = (left + right) // 2
    print(f"{indent}Check middle: arr[{mid}] = {arr[mid]}")

    if arr[mid] == target:
        print(f"{indent}Found at index {mid}!")
        return mid
    elif arr[mid] < target:
        print(f"{indent}Search right half")
        return binary_search_recurrence_demo(arr, target, mid + 1, right, depth + 1)
    else:
        print(f"{indent}Search left half")
        return binary_search_recurrence_demo(arr, target, left, mid - 1, depth + 1)

test_array = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
print("Binary search recurrence:")
result = binary_search_recurrence_demo(test_array, 13)
print(f"Result: index {result}")
```

**Binary search recurrence:** $T(n) = T(n/2) + O(1)$
- One subproblem of half the size
- Constant work to compare and determine direction

**Merge Sort Algorithm:**
Building on the merge operation from Chapter 6.4, merge sort implements the full divide-and-conquer sorting strategy:

```python-execute
def merge_sort_with_analysis(arr, depth=0):
    """Merge sort with recurrence analysis"""
    indent = "  " * depth
    print(f"{indent}merge_sort(size={len(arr)}): {arr}")

    # Base case
    if len(arr) <= 1:
        print(f"{indent}Base case: return {arr}")
        return arr, 1  # operations

    # Divide
    mid = len(arr) // 2
    left_half = arr[:mid]
    right_half = arr[mid:]
    print(f"{indent}Divide: left={left_half}, right={right_half}")

    # Conquer (recursive calls)
    left_sorted, left_ops = merge_sort_with_analysis(left_half, depth + 1)
    right_sorted, right_ops = merge_sort_with_analysis(right_half, depth + 1)

    # Combine (merge operation)
    print(f"{indent}Merge: {left_sorted} + {right_sorted}")
    merged = []
    i = j = 0
    merge_ops = 0

    while i < len(left_sorted) and j < len(right_sorted):
        merge_ops += 1
        if left_sorted[i] <= right_sorted[j]:
            merged.append(left_sorted[i])
            i += 1
        else:
            merged.append(right_sorted[j])
            j += 1

    merged.extend(left_sorted[i:])
    merged.extend(right_sorted[j:])

    total_ops = left_ops + right_ops + merge_ops
    print(f"{indent}Result: {merged} (operations: {total_ops})")
    return merged, total_ops

print("Merge sort recurrence analysis:")
test_data = [38, 27, 43, 3, 9, 82, 10]
sorted_result, total_operations = merge_sort_with_analysis(test_data)
print(f"Final result: {sorted_result}")
print(f"Total operations: {total_operations}")
```

**Merge sort recurrence:** $T(n) = 2T(n/2) + O(n)$
- Two subproblems of half the size each
- Linear work to merge the sorted halves

```table
title: "Divide-and-Conquer Algorithm Patterns"
headers: ["Algorithm", "Recurrence", "$a$", "$b$", "$f(n)$", "Decomposition Strategy"]
rows:
  - ["Binary Search", "$T(n) = T(n/2) + 1$", "1", "2", "$O(1)$", "Search one half, discard other"]
  - ["Merge Sort", "$T(n) = 2T(n/2) + n$", "2", "2", "$O(n)$", "Sort both halves, merge linearly"]
caption: "Different decomposition strategies yield different recurrence parameters"
sortable: false
```

## Systematic Analysis Methods

### The Master Theorem

The Master Theorem provides a systematic framework for analyzing divide-and-conquer recurrences of the form $T(n) = aT(n/b) + f(n)$ where $a \geq 1$, $b > 1$, and $f(n)$ is asymptotically positive.

**Theorem Statement:**

**Case 1:** If $f(n) = O(n^{\log_b a - \epsilon})$ for some $\epsilon > 0$, then:
$$
T(n) = \Theta(n^{\log_b a})
$$

**Case 2:** If $f(n) = \Theta(n^{\log_b a})$, then:
$$
T(n) = \Theta(n^{\log_b a} \log n)
$$

**Case 3:** If $f(n) = \Omega(n^{\log_b a + \epsilon})$ for some $\epsilon > 0$, and $af(n/b) \leq cf(n)$ for some $c < 1$ and sufficiently large $n$, then:
$$
T(n) = \Theta(f(n))
$$

### Application Methodology

- **Step 1:** Identify parameters $a$, $b$, and $f(n)$ from $T(n) = aT(n/b) + f(n)$
- **Step 2:** Calculate $\log_b a$
- **Step 3:** Compare $f(n)$ with $n^{\log_b a}$
- **Step 4:** Apply the appropriate case

### Worked Examples

**Example 1: Binary Search**
- Recurrence: $T(n) = T(n/2) + 1$
- Parameters: $a = 1$, $b = 2$, $f(n) = 1$
- $\log_2(1) = 0$, so $n^{\log_b a} = n^0 = 1$
- $f(n) = 1 = \Theta(n^0)$, matching Case 2
- Result: $T(n) = \Theta(n^0 \log n) = \Theta(\log n)$

**Example 2: Merge Sort**
- Recurrence: $T(n) = 2T(n/2) + n$
- Parameters: $a = 2$, $b = 2$, $f(n) = n$
- $\log_2(2) = 1$, so $n^{\log_b a} = n^1 = n$
- $f(n) = n = \Theta(n^1)$, matching Case 2
- Result: $T(n) = \Theta(n^1 \log n) = \Theta(n \log n)$

```quiz
id: master-theorem-application
question: "For the recurrence $T(n) = 4T(n/2) + n$, which Master Theorem case applies?"
options:
  - id: a
    text: "Case 1: $T(n) = \\Theta(n^2)$"
    correct: true
    explanation: "Correct! With $a=4$, $b=2$, $\\log_2(4)=2$. Since $f(n)=n = O(n^{2-1})$, Case 1 applies giving $T(n) = \\Theta(n^2)$."
  - id: b
    text: "Case 2: $T(n) = \\Theta(n^2 \\log n)$"
    correct: false
    explanation: "Case 2 requires $f(n) = \\Theta(n^{\\log_b a})$. Here $f(n)=n$ but $n^{\\log_b a}=n^2$, so Case 1 applies."
  - id: c
    text: "Case 3: $T(n) = \\Theta(n)$"
    correct: false
    explanation: "Case 3 requires $f(n)$ to dominate $n^{\\log_b a}$. Here $n < n^2$, so the recursive work dominates."
  - id: d
    text: "Master Theorem doesn't apply"
    correct: false
    explanation: "All conditions are satisfied: $a \\geq 1$, $b > 1$, and $f(n)$ is asymptotically positive."
```

## Advanced Topics and Applications

### Integer Multiplication: Karatsuba's Algorithm

Standard integer multiplication of two $n$-digit numbers requires $O(n^2)$ single-digit multiplications. Karatsuba discovered a clever divide-and-conquer approach that achieves better asymptotic performance.

**Standard multiplication approach:**
To multiply two $n$-digit numbers, we perform approximately $n^2$ single-digit multiplications and additions.

**Karatsuba's insight:**
To multiply two $n$-digit numbers $x$ and $y$, split each into high and low halves:
- $x = a \cdot 10^{n/2} + b$ (where $a$ is high half, $b$ is low half)
- $y = c \cdot 10^{n/2} + d$ (where $c$ is high half, $d$ is low half)

**Naive approach:**
$$xy = ac \cdot 10^n + (ad + bc) \cdot 10^{n/2} + bd$$
This requires 4 multiplications: $ac$, $ad$, $bc$, $bd$.

**Karatsuba's breakthrough:**
Notice that $(a + b)(c + d) = ac + ad + bc + bd$, so:
$$
ad + bc = (a + b)(c + d) - ac - bd
$$

Now we only need 3 multiplications: $ac$, $bd$, and $(a+b)(c+d)$!

```note title="Analysis Focus on Multiplication Operations"
The complexity analysis focuses on multiplication operations because they represent the computational bottleneck. For $n$-digit numbers, multiplication requires $O(n^2)$ operations while addition requires $O(n)$ operations.
```

```python-execute
def karatsuba_demo(x, y, depth=0):
    """Karatsuba multiplication with recurrence analysis"""
    indent = "  " * depth
    print(f"{indent}karatsuba({x}, {y})")

    # Base case for small numbers
    if x < 10 or y < 10:
        result = x * y
        print(f"{indent}Base case: {x} × {y} = {result}")
        return result, 1  # operations

    # Find the number of digits
    n = max(len(str(x)), len(str(y)))
    half = n // 2

    # Split the numbers
    high_x, low_x = divmod(x, 10**half)
    high_y, low_y = divmod(y, 10**half)

    print(f"{indent}Split: {x} = {high_x}*10^{half} + {low_x}")
    print(f"{indent}Split: {y} = {high_y}*10^{half} + {low_y}")

    # Three recursive multiplications
    z0, ops0 = karatsuba_demo(low_x, low_y, depth + 1)  # bd
    z2, ops2 = karatsuba_demo(high_x, high_y, depth + 1)  # ac
    z1, ops1 = karatsuba_demo(high_x + low_x, high_y + low_y, depth + 1)  # (a+b)(c+d)

    # Combine: z1 = z1 - z2 - z0 = ad + bc
    z1 = z1 - z2 - z0

    # Final result: ac * 10^n + (ad + bc) * 10^(n/2) + bd
    result = z2 * (10**(2*half)) + z1 * (10**half) + z0
    total_ops = ops0 + ops1 + ops2 + n  # +n for the combining work

    print(f"{indent}Result: {result} (operations: {total_ops})")
    return result, total_ops

print("Karatsuba multiplication analysis:")
x, y = 1234, 5678
result, ops = karatsuba_demo(x, y)
print(f"Standard multiplication: {x} × {y} = {x * y}")
print(f"Karatsuba result: {result}")
print(f"Total operations: {ops}")
```

**Karatsuba recurrence:** $T(n) = 3T(n/2) + O(n)$
- Three subproblems of half the size each
- Linear work for splitting and combining

**Master Theorem analysis:**
- Parameters: $a = 3$, $b = 2$, $f(n) = n$
- $\log_2(3) \approx 1.585$, so $n^{\log_b a} = n^{1.585}$
- $f(n) = n = O(n^{1.585-0.585})$ with $\epsilon = 0.585 > 0$, matching Case 1
- Result: $T(n) = \Theta(n^{\log_2 3}) = \Theta(n^{1.585})$

This represents a significant improvement over the standard $O(n^2)$ approach.

### Strassen's Matrix Multiplication

Strassen's algorithm demonstrates how mathematical insight can break long-standing complexity barriers. Standard matrix multiplication of two $n \times n$ matrices requires $O(n^3)$ operations.

**Standard matrix multiplication:**
$$
C_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj}
$$

**Naive divide-and-conquer approach:**
- Decompose each $n \times n$ matrix into four $(n/2) \times (n/2)$ blocks
- Compute the product using 8 recursive multiplications of smaller matrices
- Recurrence: $T(n) = 8T(n/2) + \Theta(n^2)$
- Master Theorem analysis: $\log_2(8) = 3$, $f(n) = n^2 = O(n^{3-1})$, Case 1
- Result: $T(n) = \Theta(n^3)$ (no improvement)

**Strassen's breakthrough:**
Through clever algebraic manipulation, Strassen reduced the 8 matrix multiplications to 7:

For matrices $A = \begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix}$ and $B = \begin{pmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{pmatrix}$

**Strassen's 7 products:**
- $M_1 = (A_{11} + A_{22})(B_{11} + B_{22})$
- $M_2 = (A_{21} + A_{22})B_{11}$
- $M_3 = A_{11}(B_{12} - B_{22})$
- $M_4 = A_{22}(B_{21} - B_{11})$
- $M_5 = (A_{11} + A_{12})B_{22}$
- $M_6 = (A_{21} - A_{11})(B_{11} + B_{12})$
- $M_7 = (A_{12} - A_{22})(B_{21} + B_{22})$

**Strassen recurrence:** $T(n) = 7T(n/2) + \Theta(n^2)$
- Seven subproblems of half the size each
- Quadratic work for matrix additions and subtractions

**Master Theorem analysis:**
- Parameters: $a = 7$, $b = 2$, $f(n) = n^2$
- $\log_2(7) \approx 2.807$, so $n^{\log_b a} = n^{2.807}$
- $f(n) = n^2 = O(n^{2.807-0.807})$ with $\epsilon = 0.807 > 0$, matching Case 1
- Result: $T(n) = \Theta(n^{2.807})$

```table
title: "Algorithm Complexity Comparison"
headers: ["Algorithm", "Recurrence", "Complexity", "Improvement"]
rows:
  - ["Standard Multiplication", "$O(n^2)$ directly", "$O(n^2)$", "Baseline"]
  - ["Karatsuba Multiplication", "$T(n) = 3T(n/2) + n$", "$O(n^{1.585})$", "Better than $O(n^2)$"]
  - ["Standard Matrix Mult", "$O(n^3)$ directly", "$O(n^3)$", "Baseline"]
  - ["Strassen Matrix Mult", "$T(n) = 7T(n/2) + n^2$", "$O(n^{2.807})$", "Better than $O(n^3)$"]
caption: "Mathematical insights lead to asymptotic improvements"
sortable: false
```

### Limitations and Practical Considerations

**Master Theorem limitations:**
- Only applies to recurrences of the form $T(n) = aT(n/b) + f(n)$
- Cannot handle recurrences like $T(n) = T(n-1) + T(n-2)$ (Fibonacci-type)
- Case 3 regularity condition can be difficult to verify in practice

**Practical considerations:**
- Constant factors may dominate for small problem sizes
- Implementation complexity affects practical performance
- Memory access patterns and cache effects influence real-world performance
- Strassen's algorithm, while asymptotically superior, has large constants and is typically beneficial only for very large matrices

### Mathematical vs Empirical Analysis

Mathematical analysis provides theoretical foundations that empirical testing validates and refines:

**Theoretical advantages:**
- Hardware-independent complexity characterization
- Scalability prediction beyond testable ranges
- Worst-case performance guarantees
- Algorithm comparison without implementation overhead

**Empirical validation:**
- Reveals constant factor effects that theory abstracts away
- Identifies practical crossover points between algorithms
- Validates theoretical predictions in real-world contexts

```table
title: "Analysis Method Comparison"
headers: ["Aspect", "Mathematical Analysis", "Empirical Testing", "Combined Approach"]
rows:
  - ["Scope", "Universal, scalable", "Hardware/implementation specific", "Theoretically grounded, practically validated"]
  - ["Precision", "Asymptotic behavior", "Exact performance measurement", "Both asymptotic and practical insights"]
  - ["Effort", "Mathematical derivation", "Implementation and measurement", "Initial theory, targeted validation"]
  - ["Reliability", "Provable guarantees", "Environment dependent", "Robust across contexts"]
caption: "Mathematical and empirical approaches complement each other"
sortable: false
```

## Key Takeaways

- Recurrence relations provide systematic mathematical framework for recursive algorithm analysis
- Linear recurrences typically yield $O(n)$ complexity through expansion methods
- Binary recurrences often result in exponential complexity $O(\phi^n)$ requiring sophisticated solution techniques
- The Master Theorem enables systematic analysis of divide-and-conquer algorithms following $T(n) = aT(n/b) + f(n)$
- Karatsuba and Strassen algorithms demonstrate how mathematical insight can achieve asymptotic improvements
- Binary search achieves $O(\log n)$ complexity, merge sort achieves $O(n \log n)$ complexity
- Mathematical analysis provides theoretical foundations that empirical testing validates and refines

Recurrence relation analysis transforms recursive algorithm complexity from intuitive guesswork to systematic mathematical prediction. Combined with Big O notation from Section 7.3, these tools provide a complete framework for algorithm efficiency analysis.

The next section integrates these analytical techniques through comprehensive problem-solving scenarios that demonstrate their application to real-world algorithmic decisions.